{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd852cb4d6b2452791bd0c796fd9398f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a1f85ce9cd5410ebf5328a4f36cf18c",
              "IPY_MODEL_ae21b131031f44cdbfd20a8e9bce79e4",
              "IPY_MODEL_9ab5f46a839d41b8b978d076887b43aa"
            ],
            "layout": "IPY_MODEL_b7c62b85028c4f34b75648f160f21f31"
          }
        },
        "8a1f85ce9cd5410ebf5328a4f36cf18c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a39900b89074225a8d2ca18bf8ac489",
            "placeholder": "​",
            "style": "IPY_MODEL_fcf124255d5a49178833e4f2a2bd788e",
            "value": "100%"
          }
        },
        "ae21b131031f44cdbfd20a8e9bce79e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b63d09d3b00a40149d4c288d4af686ff",
            "max": 346663984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b58c0e0a7a28479eb935e5920c960790",
            "value": 346663984
          }
        },
        "9ab5f46a839d41b8b978d076887b43aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_757992ac10a444599a85dc408f6ead2c",
            "placeholder": "​",
            "style": "IPY_MODEL_8a5df19f9b524436a866602dbaa972b3",
            "value": " 331M/331M [00:14&lt;00:00, 26.5MB/s]"
          }
        },
        "b7c62b85028c4f34b75648f160f21f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a39900b89074225a8d2ca18bf8ac489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcf124255d5a49178833e4f2a2bd788e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b63d09d3b00a40149d4c288d4af686ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b58c0e0a7a28479eb935e5920c960790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "757992ac10a444599a85dc408f6ead2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5df19f9b524436a866602dbaa972b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Transcribing Speech-to-text**"
      ],
      "metadata": {
        "id": "z5nJuzrD7-Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our AI video Editor Application,\n",
        "\n",
        "The Initial step is find out the current state of the art system for transcribing Speech to Text "
      ],
      "metadata": {
        "id": "L4EdKE7z8Hy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paper Link: <b> Robust Speech Recognition via Large-Scale Weak Supervision </b>\n",
        "\n",
        "https://cdn.openai.com/papers/whisper.pdf"
      ],
      "metadata": {
        "id": "s-UucAvF_ncR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We intend to incorporate OpenAI whisper for our AI Product post conducting extensive research and reading numerous journals.\n",
        "\n",
        "*Its a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.*"
      ],
      "metadata": {
        "id": "MyIwsMmA83Tu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/openai/whisper/main/approach.png\"></img>"
      ],
      "metadata": {
        "id": "Acqf12NG90mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Various Whisper Models**\n",
        "\n",
        "There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed.\n",
        "\n",
        "\n",
        "For English-only applications, the .en models tend to perform better, especially for the tiny.en and base.en models. We observed that the difference becomes less significant for the small.en and medium.en models.\n",
        "\n",
        "\n",
        "tiny\t39 M\ttiny.en\ttiny\t~1 GB\t~32x\n",
        "base\t74 M\tbase.en\tbase\t~1 GB\t~16x\n",
        "small\t244 M\tsmall.en\tsmall\t~2 GB\t~6x\n",
        "medium\t769 M\tmedium.en\tmedium\t~5 GB\t~2x\n",
        "large\t1550 M\tN/A\tlarge\t~10 GB\t1x\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/openai/whisper/main/language-breakdown.svg\"> </img>"
      ],
      "metadata": {
        "id": "eRIVF8up_Y5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Necessary Libraries**"
      ],
      "metadata": {
        "id": "mNyw-SkT-NJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the new source binaries from Whisper Repo\n",
        "!pip install git+https://github.com/openai/whisper.git "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOUWvWBb-Pfb",
        "outputId": "1b0faef2-a5b8-4e4a-d7c0-516537de2156"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-9_lj13qz\n",
            "  Running command git clone -q https://github.com/openai/whisper.git /tmp/pip-req-build-9_lj13qz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python==0.2.0->whisper==1.0) (0.16.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.19.0->whisper==1.0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.19.0->whisper==1.0) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (3.0.4)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.0-py3-none-any.whl size=1175218 sha256=1d121f785ac971328e1f0d19a70d939114a799d7278faaaad4b6943361907d36\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pjym_mcj/wheels/16/15/89/1c7bb31bd0006793a95549d04785121a8a36daad9158e1e43a\n",
            "Successfully built whisper\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, ffmpeg-python, whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0 whisper-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#updating apt and installing the ffmeg modules for working with videos and audios\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibEsfJ9E-m4k",
        "outputId": "3e36dc84-1ee7-4740-ee56-9ce5b14d9608"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 14.2 kB/88.\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [101 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,040 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,554 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,217 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,472 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,134 kB]\n",
            "Fetched 14.1 MB in 3s (4,261 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "17 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing setup rust\n",
        "!pip install setuptools-rust"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "m6-8I3hy-x7G",
        "outputId": "121d1ba2-1e3a-46ee-9648-22d347fb0d84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools-rust\n",
            "  Downloading setuptools_rust-1.5.2-py3-none-any.whl (23 kB)\n",
            "Collecting semantic-version<3,>=2.8.2\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from setuptools-rust) (4.1.1)\n",
            "Collecting setuptools>=62.4\n",
            "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools, semantic-version, setuptools-rust\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "Successfully installed semantic-version-2.10.0 setuptools-65.5.1 setuptools-rust-1.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing imageio==2.4.1 dependency\n",
        "! pip install imageio==2.4.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3nNEFM--5hF",
        "outputId": "7ad9515d-116e-472e-b495-61d45a0d0b8a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio==2.4.1\n",
            "  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.1) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.1) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303887 sha256=d27644bf41e63d93f218fc2e41c8daa8c27e9173883b2a1b87c3917f5036cf21\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/20/07/7bb9c8c44e6ec2efa60fd0e6280094f53f65f41767ef69a5ee\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "Successfully installed imageio-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Whisper Module**"
      ],
      "metadata": {
        "id": "lvjhVzC1_CQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper"
      ],
      "metadata": {
        "id": "mcicPBY3_A3y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Record audio to test**"
      ],
      "metadata": {
        "id": "DYhK4MTRBCJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from io import BytesIO\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "from IPython.display import Javascript\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "recorder = new MediaRecorder(stream)\n",
        "chunks = []\n",
        "recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=3):\n",
        "  print(\"\")\n",
        "  print(\"Speak Now...\")\n",
        "  display(Javascript(RECORD))\n",
        "  sec += 1\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  print(\"Done Recording !\")\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  return b #byte stream"
      ],
      "metadata": {
        "id": "4sPhnqzTBF2l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run cell and record speaking for 5 seconds\n",
        "audio = record(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "U2F5GpPJBzXz",
        "outputId": "fe79d8e9-5801-44da-a439-19f4c0f85488"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Speak Now...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "recorder = new MediaRecorder(stream)\n",
              "chunks = []\n",
              "recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Recording !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display user audio recordings\n",
        "import IPython.display as ipd\n",
        "\n",
        "ipd.display(ipd.Audio(audio))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "t1UUknKcB6eN",
        "outputId": "b04a360d-3845-41a6-dd04-86e0c60c33c5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tDvnIZgD88Q",
        "outputId": "7ff527ff-1411-46b4-b688-fee48836c9ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bytes"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#doing transcription -  pythonic way\n",
        "import whisper\n",
        "\n",
        "# loading the base model right now to test.\n",
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "KrYXWMTm_cQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ffc686-9e77-4649-a571-4d801489c328"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 53.4MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to tmp audio file to test with openai whisper\n",
        "def converttoaudiofile(audio:bytes):\n",
        "  '''\n",
        "  convert in memory bytes to a temp files\n",
        "  ''' \n",
        "  try:\n",
        "    with open(\"test.mp3\",mode=\"wb\") as f:\n",
        "      f.write(audio)\n",
        "  except Exception as e:\n",
        "    print(\"some issues in writing files\",e)\n",
        "    return -1,\"some issues\"\n",
        "  return 1,\"successful\""
      ],
      "metadata": {
        "id": "bDOvm_x-DiMF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converttoaudiofile(audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0k4TzYEE27B",
        "outputId": "15cdccf3-7594-4507-def3-bfa880bc3096"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 'successful')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the tmp file\n",
        "# display user audio recordings\n",
        "import IPython.display as ipd\n",
        "\n",
        "ipd.display(ipd.Audio(\"test.mp3\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "gPU6vfydFMeS",
        "outputId": "551fe926-b229-4538-cb39-35aa114f8163"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting the transcription result\n",
        "result = model.transcribe(\"test.mp3\",fp16=False, language='English')"
      ],
      "metadata": {
        "id": "hV0X8q9jA4xI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the transcription\n",
        "result[\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6zeFRnhoDyI9",
        "outputId": "9e9d42b9-c0bd-416b-d7bc-f5617183f580"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***the transcription works well..!!!***"
      ],
      "metadata": {
        "id": "kNDPEtiNF6ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the base model on famous LibriSpeech dataset**"
      ],
      "metadata": {
        "id": "B_Npq2LcF_SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLPbkeAIL1X",
        "outputId": "3ff2acb0-c5f3-4f58-d7df-744e88fa1734"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.6 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 43.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading libraries\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "#check if cuda is present and assign gpu run\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class LibriSpeech(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n",
        "    It will drop the last few seconds of a very small portion of the utterances.\n",
        "    \"\"\"\n",
        "    def __init__(self, split=\"test-clean\", device=DEVICE):\n",
        "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
        "            root=os.path.expanduser(\"~/.cache\"),\n",
        "            url=split,\n",
        "            download=True,\n",
        "        )\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
        "        assert sample_rate == 16000\n",
        "        audio = whisper.pad_or_trim(audio.flatten()).to(self.device)\n",
        "        mel = whisper.log_mel_spectrogram(audio)\n",
        "        \n",
        "        return (mel, text)\n",
        "dataset = LibriSpeech(\"test-clean\")\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bd852cb4d6b2452791bd0c796fd9398f",
            "8a1f85ce9cd5410ebf5328a4f36cf18c",
            "ae21b131031f44cdbfd20a8e9bce79e4",
            "9ab5f46a839d41b8b978d076887b43aa",
            "b7c62b85028c4f34b75648f160f21f31",
            "8a39900b89074225a8d2ca18bf8ac489",
            "fcf124255d5a49178833e4f2a2bd788e",
            "b63d09d3b00a40149d4c288d4af686ff",
            "b58c0e0a7a28479eb935e5920c960790",
            "757992ac10a444599a85dc408f6ead2c",
            "8a5df19f9b524436a866602dbaa972b3"
          ]
        },
        "id": "_r1B7xwmGB3G",
        "outputId": "4db56762-7067-49ca-b6d7-3965a373d851"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/331M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd852cb4d6b2452791bd0c796fd9398f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "dataset = requests.get(\"https://datasets-server.huggingface.co/first-rows?dataset=librispeech_asr&config=clean&split=test\")"
      ],
      "metadata": {
        "id": "Hn-T7E4KKh-0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.loads(dataset.text)"
      ],
      "metadata": {
        "id": "5IiTSSXWLHZ_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = data['rows'][:20]"
      ],
      "metadata": {
        "id": "ZhPkQyYTMQl7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data"
      ],
      "metadata": {
        "id": "3i56Xh-0NWOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c73ef6b-bce3-4190-cce2-9da129286169"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'row_idx': 0,\n",
              "  'row': {'file': '6930-75918-0000.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/0/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/0/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0000'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 1,\n",
              "  'row': {'file': '6930-75918-0001.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/1/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/1/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'THE ENGLISH FORWARDED TO THE FRENCH BASKETS OF FLOWERS OF WHICH THEY HAD MADE A PLENTIFUL PROVISION TO GREET THE ARRIVAL OF THE YOUNG PRINCESS THE FRENCH IN RETURN INVITED THE ENGLISH TO A SUPPER WHICH WAS TO BE GIVEN THE NEXT DAY',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0001'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 2,\n",
              "  'row': {'file': '6930-75918-0002.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/2/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/2/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'CONGRATULATIONS WERE POURED IN UPON THE PRINCESS EVERYWHERE DURING HER JOURNEY',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0002'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 3,\n",
              "  'row': {'file': '6930-75918-0003.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/3/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/3/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'FROM THE RESPECT PAID HER ON ALL SIDES SHE SEEMED LIKE A QUEEN AND FROM THE ADORATION WITH WHICH SHE WAS TREATED BY TWO OR THREE SHE APPEARED AN OBJECT OF WORSHIP THE QUEEN MOTHER GAVE THE FRENCH THE MOST AFFECTIONATE RECEPTION FRANCE WAS HER NATIVE COUNTRY AND SHE HAD SUFFERED TOO MUCH UNHAPPINESS IN ENGLAND FOR ENGLAND TO HAVE MADE HER FORGET FRANCE',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0003'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 4,\n",
              "  'row': {'file': '6930-75918-0004.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/4/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/4/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'SHE TAUGHT HER DAUGHTER THEN BY HER OWN AFFECTION FOR IT THAT LOVE FOR A COUNTRY WHERE THEY HAD BOTH BEEN HOSPITABLY RECEIVED AND WHERE A BRILLIANT FUTURE OPENED BEFORE THEM',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0004'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 5,\n",
              "  'row': {'file': '6930-75918-0005.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/5/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/5/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'THE COUNT HAD THROWN HIMSELF BACK ON HIS SEAT LEANING HIS SHOULDERS AGAINST THE PARTITION OF THE TENT AND REMAINED THUS HIS FACE BURIED IN HIS HANDS WITH HEAVING CHEST AND RESTLESS LIMBS',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0005'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 6,\n",
              "  'row': {'file': '6930-75918-0006.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/6/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/6/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'THIS HAS INDEED BEEN A HARASSING DAY CONTINUED THE YOUNG MAN HIS EYES FIXED UPON HIS FRIEND',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0006'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 7,\n",
              "  'row': {'file': '6930-75918-0007.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/7/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/7/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'YOU WILL BE FRANK WITH ME I ALWAYS AM',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0007'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 8,\n",
              "  'row': {'file': '6930-75918-0008.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/8/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/8/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'CAN YOU IMAGINE WHY BUCKINGHAM HAS BEEN SO VIOLENT I SUSPECT',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0008'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 9,\n",
              "  'row': {'file': '6930-75918-0009.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/9/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/9/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'IT IS YOU WHO ARE MISTAKEN RAOUL I HAVE READ HIS DISTRESS IN HIS EYES IN HIS EVERY GESTURE AND ACTION THE WHOLE DAY',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0009'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 10,\n",
              "  'row': {'file': '6930-75918-0010.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/10/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/10/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'I CAN PERCEIVE LOVE CLEARLY ENOUGH',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0010'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 11,\n",
              "  'row': {'file': '6930-75918-0011.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/11/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/11/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'I AM CONVINCED OF WHAT I SAY SAID THE COUNT',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0011'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 12,\n",
              "  'row': {'file': '6930-75918-0012.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/12/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/12/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'IT IS ANNOYANCE THEN',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0012'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 13,\n",
              "  'row': {'file': '6930-75918-0013.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/13/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/13/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'IN THOSE VERY TERMS I EVEN ADDED MORE',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0013'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 14,\n",
              "  'row': {'file': '6930-75918-0014.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/14/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/14/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'BUT CONTINUED RAOUL NOT INTERRUPTED BY THIS MOVEMENT OF HIS FRIEND HEAVEN BE PRAISED THE FRENCH WHO ARE PRONOUNCED TO BE THOUGHTLESS AND INDISCREET RECKLESS EVEN ARE CAPABLE OF BRINGING A CALM AND SOUND JUDGMENT TO BEAR ON MATTERS OF SUCH HIGH IMPORTANCE',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0014'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 15,\n",
              "  'row': {'file': '6930-75918-0015.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/15/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/15/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': \"THUS IT IS THAT THE HONOR OF THREE IS SAVED OUR COUNTRY'S OUR MASTER'S AND OUR OWN\",\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0015'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 16,\n",
              "  'row': {'file': '6930-75918-0016.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/16/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/16/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'YES I NEED REPOSE MANY THINGS HAVE AGITATED ME TO DAY BOTH IN MIND AND BODY WHEN YOU RETURN TO MORROW I SHALL NO LONGER BE THE SAME MAN',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0016'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 17,\n",
              "  'row': {'file': '6930-75918-0017.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/17/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/17/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'BUT IN THIS FRIENDLY PRESSURE RAOUL COULD DETECT THE NERVOUS AGITATION OF A GREAT INTERNAL CONFLICT',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0017'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 18,\n",
              "  'row': {'file': '6930-75918-0018.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/18/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/18/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'THE NIGHT WAS CLEAR STARLIT AND SPLENDID THE TEMPEST HAD PASSED AWAY AND THE SWEET INFLUENCES OF THE EVENING HAD RESTORED LIFE PEACE AND SECURITY EVERYWHERE',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0018'},\n",
              "  'truncated_cells': []},\n",
              " {'row_idx': 19,\n",
              "  'row': {'file': '6930-75918-0019.flac',\n",
              "   'audio': [{'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/19/audio/audio.mp3',\n",
              "     'type': 'audio/mpeg'},\n",
              "    {'src': 'https://datasets-server.huggingface.co/assets/librispeech_asr/--/clean/test/19/audio/audio.wav',\n",
              "     'type': 'audio/wav'}],\n",
              "   'text': 'UPON THE LARGE SQUARE IN FRONT OF THE HOTEL THE SHADOWS OF THE TENTS INTERSECTED BY THE GOLDEN MOONBEAMS FORMED AS IT WERE A HUGE MOSAIC OF JET AND YELLOW FLAGSTONES',\n",
              "   'speaker_id': 6930,\n",
              "   'chapter_id': 75918,\n",
              "   'id': '6930-75918-0019'},\n",
              "  'truncated_cells': []}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(pd.json_normalize(sample_data),orient='columns')"
      ],
      "metadata": {
        "id": "FPUYzcpjLw6G"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jGBmSLatND2_",
        "outputId": "f49f3f77-8173-4ea2-9996-dd57be0ef850"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    row_idx truncated_cells              row.file  \\\n",
              "0         0              []  6930-75918-0000.flac   \n",
              "1         1              []  6930-75918-0001.flac   \n",
              "2         2              []  6930-75918-0002.flac   \n",
              "3         3              []  6930-75918-0003.flac   \n",
              "4         4              []  6930-75918-0004.flac   \n",
              "5         5              []  6930-75918-0005.flac   \n",
              "6         6              []  6930-75918-0006.flac   \n",
              "7         7              []  6930-75918-0007.flac   \n",
              "8         8              []  6930-75918-0008.flac   \n",
              "9         9              []  6930-75918-0009.flac   \n",
              "10       10              []  6930-75918-0010.flac   \n",
              "11       11              []  6930-75918-0011.flac   \n",
              "12       12              []  6930-75918-0012.flac   \n",
              "13       13              []  6930-75918-0013.flac   \n",
              "14       14              []  6930-75918-0014.flac   \n",
              "15       15              []  6930-75918-0015.flac   \n",
              "16       16              []  6930-75918-0016.flac   \n",
              "17       17              []  6930-75918-0017.flac   \n",
              "18       18              []  6930-75918-0018.flac   \n",
              "19       19              []  6930-75918-0019.flac   \n",
              "\n",
              "                                            row.audio  \\\n",
              "0   [{'src': 'https://datasets-server.huggingface....   \n",
              "1   [{'src': 'https://datasets-server.huggingface....   \n",
              "2   [{'src': 'https://datasets-server.huggingface....   \n",
              "3   [{'src': 'https://datasets-server.huggingface....   \n",
              "4   [{'src': 'https://datasets-server.huggingface....   \n",
              "5   [{'src': 'https://datasets-server.huggingface....   \n",
              "6   [{'src': 'https://datasets-server.huggingface....   \n",
              "7   [{'src': 'https://datasets-server.huggingface....   \n",
              "8   [{'src': 'https://datasets-server.huggingface....   \n",
              "9   [{'src': 'https://datasets-server.huggingface....   \n",
              "10  [{'src': 'https://datasets-server.huggingface....   \n",
              "11  [{'src': 'https://datasets-server.huggingface....   \n",
              "12  [{'src': 'https://datasets-server.huggingface....   \n",
              "13  [{'src': 'https://datasets-server.huggingface....   \n",
              "14  [{'src': 'https://datasets-server.huggingface....   \n",
              "15  [{'src': 'https://datasets-server.huggingface....   \n",
              "16  [{'src': 'https://datasets-server.huggingface....   \n",
              "17  [{'src': 'https://datasets-server.huggingface....   \n",
              "18  [{'src': 'https://datasets-server.huggingface....   \n",
              "19  [{'src': 'https://datasets-server.huggingface....   \n",
              "\n",
              "                                             row.text  row.speaker_id  \\\n",
              "0      CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS            6930   \n",
              "1   THE ENGLISH FORWARDED TO THE FRENCH BASKETS OF...            6930   \n",
              "2   CONGRATULATIONS WERE POURED IN UPON THE PRINCE...            6930   \n",
              "3   FROM THE RESPECT PAID HER ON ALL SIDES SHE SEE...            6930   \n",
              "4   SHE TAUGHT HER DAUGHTER THEN BY HER OWN AFFECT...            6930   \n",
              "5   THE COUNT HAD THROWN HIMSELF BACK ON HIS SEAT ...            6930   \n",
              "6   THIS HAS INDEED BEEN A HARASSING DAY CONTINUED...            6930   \n",
              "7               YOU WILL BE FRANK WITH ME I ALWAYS AM            6930   \n",
              "8   CAN YOU IMAGINE WHY BUCKINGHAM HAS BEEN SO VIO...            6930   \n",
              "9   IT IS YOU WHO ARE MISTAKEN RAOUL I HAVE READ H...            6930   \n",
              "10                 I CAN PERCEIVE LOVE CLEARLY ENOUGH            6930   \n",
              "11        I AM CONVINCED OF WHAT I SAY SAID THE COUNT            6930   \n",
              "12                               IT IS ANNOYANCE THEN            6930   \n",
              "13              IN THOSE VERY TERMS I EVEN ADDED MORE            6930   \n",
              "14  BUT CONTINUED RAOUL NOT INTERRUPTED BY THIS MO...            6930   \n",
              "15  THUS IT IS THAT THE HONOR OF THREE IS SAVED OU...            6930   \n",
              "16  YES I NEED REPOSE MANY THINGS HAVE AGITATED ME...            6930   \n",
              "17  BUT IN THIS FRIENDLY PRESSURE RAOUL COULD DETE...            6930   \n",
              "18  THE NIGHT WAS CLEAR STARLIT AND SPLENDID THE T...            6930   \n",
              "19  UPON THE LARGE SQUARE IN FRONT OF THE HOTEL TH...            6930   \n",
              "\n",
              "    row.chapter_id           row.id  \n",
              "0            75918  6930-75918-0000  \n",
              "1            75918  6930-75918-0001  \n",
              "2            75918  6930-75918-0002  \n",
              "3            75918  6930-75918-0003  \n",
              "4            75918  6930-75918-0004  \n",
              "5            75918  6930-75918-0005  \n",
              "6            75918  6930-75918-0006  \n",
              "7            75918  6930-75918-0007  \n",
              "8            75918  6930-75918-0008  \n",
              "9            75918  6930-75918-0009  \n",
              "10           75918  6930-75918-0010  \n",
              "11           75918  6930-75918-0011  \n",
              "12           75918  6930-75918-0012  \n",
              "13           75918  6930-75918-0013  \n",
              "14           75918  6930-75918-0014  \n",
              "15           75918  6930-75918-0015  \n",
              "16           75918  6930-75918-0016  \n",
              "17           75918  6930-75918-0017  \n",
              "18           75918  6930-75918-0018  \n",
              "19           75918  6930-75918-0019  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b22453f9-0c53-497d-83d9-9118f5e3b7b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_idx</th>\n",
              "      <th>truncated_cells</th>\n",
              "      <th>row.file</th>\n",
              "      <th>row.audio</th>\n",
              "      <th>row.text</th>\n",
              "      <th>row.speaker_id</th>\n",
              "      <th>row.chapter_id</th>\n",
              "      <th>row.id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0000.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0001.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>THE ENGLISH FORWARDED TO THE FRENCH BASKETS OF...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0002.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>CONGRATULATIONS WERE POURED IN UPON THE PRINCE...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0003.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>FROM THE RESPECT PAID HER ON ALL SIDES SHE SEE...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0004.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>SHE TAUGHT HER DAUGHTER THEN BY HER OWN AFFECT...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0005.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>THE COUNT HAD THROWN HIMSELF BACK ON HIS SEAT ...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0006.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>THIS HAS INDEED BEEN A HARASSING DAY CONTINUED...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0007.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>YOU WILL BE FRANK WITH ME I ALWAYS AM</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0008.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>CAN YOU IMAGINE WHY BUCKINGHAM HAS BEEN SO VIO...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0009.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>IT IS YOU WHO ARE MISTAKEN RAOUL I HAVE READ H...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0010.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>I CAN PERCEIVE LOVE CLEARLY ENOUGH</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0011.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>I AM CONVINCED OF WHAT I SAY SAID THE COUNT</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0012.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>IT IS ANNOYANCE THEN</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0013.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>IN THOSE VERY TERMS I EVEN ADDED MORE</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0014.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>BUT CONTINUED RAOUL NOT INTERRUPTED BY THIS MO...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0015.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>THUS IT IS THAT THE HONOR OF THREE IS SAVED OU...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0016.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>YES I NEED REPOSE MANY THINGS HAVE AGITATED ME...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0017.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>BUT IN THIS FRIENDLY PRESSURE RAOUL COULD DETE...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0018.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>THE NIGHT WAS CLEAR STARLIT AND SPLENDID THE T...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>[]</td>\n",
              "      <td>6930-75918-0019.flac</td>\n",
              "      <td>[{'src': 'https://datasets-server.huggingface....</td>\n",
              "      <td>UPON THE LARGE SQUARE IN FRONT OF THE HOTEL TH...</td>\n",
              "      <td>6930</td>\n",
              "      <td>75918</td>\n",
              "      <td>6930-75918-0019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b22453f9-0c53-497d-83d9-9118f5e3b7b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b22453f9-0c53-497d-83d9-9118f5e3b7b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b22453f9-0c53-497d-83d9-9118f5e3b7b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the base en language model\n",
        "\n",
        "model = whisper.load_model(\"base.en\")\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IDg6-pxGsLz",
        "outputId": "b5cadd4b-46a9-4945-c03e-a8808f86341e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:04<00:00, 29.5MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is English-only and has 71,825,408 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pso0CRW3MTtJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NOTE: For Increased and most efficient transcription, we are incorporating the medium model***"
      ],
      "metadata": {
        "id": "3sidnYxtPozL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenAI whisper does not provide word timestamps**"
      ],
      "metadata": {
        "id": "bDVe3qN1OIEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stabilizing TImestamps and generating Word timestamps for our application.**"
      ],
      "metadata": {
        "id": "azLUhNc4OB0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile stable_whisper.py\n",
        "\n",
        "import ffmpeg\n",
        "import whisper\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions import Categorical\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from whisper.audio import SAMPLE_RATE, N_FRAMES, HOP_LENGTH, pad_or_trim, log_mel_spectrogram\n",
        "from whisper.decoding import DecodingOptions, DecodingResult\n",
        "from whisper.tokenizer import LANGUAGES\n",
        "from whisper.utils import exact_div, format_timestamp, compression_ratio\n",
        "from whisper.model import Whisper\n",
        "from whisper.decoding import DecodingTask, BeamSearchDecoder, GreedyDecoder\n",
        "from whisper.tokenizer import Tokenizer, get_tokenizer\n",
        "from types import MethodType\n",
        "from itertools import chain, repeat\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "# no_caption changed to no_speech newer commits\n",
        "def get_new_attrs(obj_, attr: str):\n",
        "    if attr == 'no_caption_probs':\n",
        "        return getattr(obj_, attr) if hasattr(obj_, 'no_caption_probs') else getattr(obj_, 'no_speech_probs')\n",
        "    elif attr == 'no_caption_prob':\n",
        "        return getattr(obj_, attr) if hasattr(obj_, 'no_caption_prob') else getattr(obj_, 'no_speech_prob')\n",
        "    elif attr == 'no_captions':\n",
        "        return getattr(obj_, attr) if hasattr(obj_, 'no_captions') else getattr(obj_, 'no_speech')\n",
        "    else:\n",
        "        raise NotImplementedError(attr)\n",
        "\n",
        "\n",
        "def check_ascending_sequence(seq: Union[List[Union[int, float]], np.ndarray], verbose=True) -> bool:\n",
        "    \"\"\"\n",
        "    check if a sequence of numbers are in ascending order\n",
        "    \"\"\"\n",
        "    is_ascending = True\n",
        "    for idx, (i, j) in enumerate(zip(seq[:-1], seq[1:])):\n",
        "        if i > j:\n",
        "            is_ascending = False\n",
        "            if verbose:\n",
        "                print(f'[Index{idx}]:{i} > [Index{idx + 1}]:{j}')\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    return is_ascending\n",
        "\n",
        "\n",
        "def check_ascending_sentence_ts(res: (dict, list)) -> bool:\n",
        "    segs = res['segments'] if isinstance(res, dict) else res\n",
        "    return check_ascending_sequence(list(chain.from_iterable((float(i['start']), float(i['end']))\n",
        "                                                             for i in segs)))\n",
        "\n",
        "\n",
        "def check_ascending_word_ts(res: (dict, list)) -> bool:\n",
        "    cc = group_word_timestamps(res['segments'] if isinstance(res, dict) else res, ts_key='word_timestamps')\n",
        "    return check_ascending_sequence((list(chain.from_iterable((float(i['start']), float(i['end']))\n",
        "                                                              for i in cc))))\n",
        "\n",
        "\n",
        "def is_equal_ts(a: (float, int, np.ndarray), b: (float, int, np.ndarray), rtol=1e-03):\n",
        "    \"\"\"\n",
        "    check if timestamp a and timestamp b are equal within the relative tolerance (rtol)\n",
        "    \"\"\"\n",
        "    return np.isclose(a, b, rtol=rtol)\n",
        "\n",
        "\n",
        "def check_is_same_results(res0: (dict, list), res1: (dict, list), check_unstable=False) -> bool:\n",
        "    \"\"\"\n",
        "    check if res0 and res1 have same timestamps\n",
        "    \"\"\"\n",
        "    if isinstance(res0, dict):\n",
        "        res0 = res0['segments']\n",
        "    if isinstance(res1, dict):\n",
        "        res1 = res1['segments']\n",
        "    ts_key = 'unstable_word_timestamps' if check_unstable else 'word_timestamps'\n",
        "    inner_ts_key = 'timestamps' if check_unstable else 'timestamp'\n",
        "\n",
        "    def _reduce(x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            return set(tuple(x)) == {True}\n",
        "        return x\n",
        "\n",
        "    t = set(set(_reduce(is_equal_ts(a[inner_ts_key], b[inner_ts_key])) for a, b in zip(i[ts_key], j[ts_key])) == {True}\n",
        "            for i, j in zip(res0['segments'], res1['segments']))\n",
        "    return t == {True}\n",
        "\n",
        "\n",
        "def to_srt(lines: List[dict], save_path: str = None, strip=False) -> str:\n",
        "    \"\"\"\n",
        "    lines: List[dict]\n",
        "        [{start:<start-timestamp-of-text>, end:<end-timestamp-of-text>, text:<str-of-text>}, ...]\n",
        "    \"\"\"\n",
        "\n",
        "    def secs_to_hhmmss(secs: (float, int)):\n",
        "        mm, ss = divmod(secs, 60)\n",
        "        hh, mm = divmod(mm, 60)\n",
        "        return f'{hh:0>2.0f}:{mm:0>2.0f}:{ss:0>6.3f}'.replace(\".\", \",\")\n",
        "\n",
        "    srt_str = '\\n'.join(\n",
        "        f'{i}\\n'\n",
        "        f'{secs_to_hhmmss(sub[\"start\"])} --> {secs_to_hhmmss(sub[\"end\"])}\\n'\n",
        "        f'{sub[\"text\"].strip() if strip else sub[\"text\"]}\\n'\n",
        "        for i, sub in enumerate(lines, 1))\n",
        "\n",
        "    if save_path:\n",
        "        with open(save_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(srt_str)\n",
        "        print(f'Saved: {os.path.abspath(save_path)}')\n",
        "\n",
        "    return srt_str\n",
        "\n",
        "\n",
        "def group_word_timestamps(res: (dict, list), one_group=True, combine_compound=False,\n",
        "                          ts_key='whole_word_timestamps', min_dur: float = None):\n",
        "\n",
        "    if min_dur is None:\n",
        "        min_dur = 0.02\n",
        "\n",
        "    def group_ts(ts_: List[dict], start) -> List[dict]:\n",
        "        first_group: List[dict] = []\n",
        "        for w_ts in ts_:\n",
        "            if first_group:\n",
        "                if (not combine_compound or w_ts['word'].startswith(' ')) and \\\n",
        "                        (w_ts['timestamp'] - first_group[-1]['start']) >= min_dur and \\\n",
        "                        first_group[-1]['end'] < w_ts['timestamp']:\n",
        "                    first_group.append(dict(start=first_group[-1]['end'],\n",
        "                                            end=w_ts['timestamp'],\n",
        "                                            text=w_ts['word']))\n",
        "                else:\n",
        "                    first_group[-1]['end'] = max(first_group[-1]['end'], w_ts['timestamp'])\n",
        "                    first_group[-1]['text'] += w_ts['word']\n",
        "            else:\n",
        "                first_group.append(dict(start=start,\n",
        "                                        end=w_ts['timestamp'],\n",
        "                                        text=w_ts['word']))\n",
        "\n",
        "        return first_group\n",
        "\n",
        "    def group_zero_duration(first_group: List[dict]) -> List[dict]:\n",
        "        final_group: List[dict] = []\n",
        "        for ts_dict in first_group:\n",
        "            if not final_group or (ts_dict['end'] - ts_dict['start']) > 0:\n",
        "                final_group.append(ts_dict)\n",
        "            else:\n",
        "                final_group[-1]['end'] = ts_dict['end']\n",
        "                final_group[-1]['text'] += ts_dict['text']\n",
        "\n",
        "        return final_group\n",
        "\n",
        "    segs: List[dict] = res['segments'] if isinstance(res, dict) else res\n",
        "    assert set(ts_key in seg for seg in segs) == {True}, f'input contains missing {ts_key}'\n",
        "\n",
        "    grouped = (group_ts(seg[ts_key], seg['start']) for seg in segs)\n",
        "    return group_zero_duration(list(chain.from_iterable(grouped))) if one_group else list(grouped)\n",
        "\n",
        "\n",
        "def tighten_timestamps(res: dict, end_at_last_word=True, end_before_period=False, start_at_first_word=False) -> dict:\n",
        "    res = deepcopy(res)\n",
        "    for i in range(len(res['segments'])):\n",
        "        if start_at_first_word:\n",
        "            res['segments'][i]['start'] = res['segments'][i]['word_timestamps'][0]['timestamp']\n",
        "        if end_before_period and \\\n",
        "                res['segments'][i]['word_timestamps'][-1] == '.' and \\\n",
        "                len(res['segments'][i]['word_timestamps']) > 1:\n",
        "            res['segments'][i]['end'] = res['segments'][i]['word_timestamps'][-2]['timestamp']\n",
        "        elif end_at_last_word:\n",
        "            res['segments'][i]['end'] = res['segments'][i]['word_timestamps'][-1]['timestamp']\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def results_to_srt(res: dict, srt_path, word_level=True, combine_compound=False,\n",
        "                   end_at_last_word=False, end_before_period=False, start_at_first_word=False, strip=False):\n",
        "    if word_level:\n",
        "        results_to_word_srt(res, srt_path, combine_compound=combine_compound, strip=strip)\n",
        "    else:\n",
        "        results_to_sentence_srt(res, srt_path,\n",
        "                                end_at_last_word=end_at_last_word,\n",
        "                                end_before_period=end_before_period,\n",
        "                                start_at_first_word=start_at_first_word,\n",
        "                                strip=strip)\n",
        "\n",
        "\n",
        "def results_to_sentence_srt(res: dict, srt_path,\n",
        "                            end_at_last_word=False,\n",
        "                            end_before_period=False,\n",
        "                            start_at_first_word=False,\n",
        "                            strip=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    res: dict\n",
        "        results from modified model\n",
        "    srt_path: str\n",
        "        output path of srt\n",
        "    end_at_last_word: bool\n",
        "        set end-of-sentence to timestamp-of-last-token\n",
        "    end_before_period: bool\n",
        "        set end-of-sentence to timestamp-of-last-non-period-token\n",
        "    start_at_first_word: bool\n",
        "        set start-of-sentence to timestamp-of-first-token\n",
        "    strip: bool\n",
        "        perform strip() on each sentence\n",
        "\n",
        "    \"\"\"\n",
        "    strict = any((end_at_last_word, end_before_period, start_at_first_word))\n",
        "    segs = tighten_timestamps(res,\n",
        "                              end_at_last_word=end_at_last_word,\n",
        "                              end_before_period=end_before_period,\n",
        "                              start_at_first_word=start_at_first_word)['segments'] \\\n",
        "        if strict else res['segments']\n",
        "\n",
        "    max_idx = len(segs) - 1\n",
        "    i = 1\n",
        "    while i <= max_idx:\n",
        "        if not (segs[i]['end'] - segs[i]['start']):\n",
        "            if segs[i - 1]['end'] == segs[i]['end']:\n",
        "                segs[i - 1]['text'] += (' ' + segs[i]['text'].strip())\n",
        "                del segs[i]\n",
        "                max_idx -= 1\n",
        "                continue\n",
        "            else:\n",
        "                segs[i]['start'] = segs[i - 1]['end']\n",
        "        i += 1\n",
        "\n",
        "    to_srt(segs, srt_path, strip=strip)\n",
        "\n",
        "\n",
        "def results_to_word_srt(res: dict, srt_path, combine_compound=False, strip=False, min_dur: float = None):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    res: dict\n",
        "        results from modified model\n",
        "    srt_path: str\n",
        "        output path of srt\n",
        "    combine_compound: bool\n",
        "        concatenate words without inbetween spacing\n",
        "    strip: bool\n",
        "        perform strip() on each word\n",
        "    min_dur: bool\n",
        "        minimum duration for each word (i.e. concat the words if it is less than specified value; Default 0.02)\n",
        "\n",
        "    \"\"\"\n",
        "    to_srt(group_word_timestamps(res, combine_compound=combine_compound, min_dur=min_dur),\n",
        "           srt_path, strip=strip)\n",
        "\n",
        "\n",
        "def results_to_token_srt(res: dict, srt_path, combine_compound=False, strip=False, min_dur: float = None):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    res: dict\n",
        "        results from modified model\n",
        "    srt_path: str\n",
        "        output path of srt\n",
        "    combine_compound: bool\n",
        "        concatenate words without inbetween spacing\n",
        "    strip: bool\n",
        "        perform strip() on each token\n",
        "    min_dur: bool\n",
        "        minimum duration for each token (i.e. concat the tokens if it is less than specified value; Default 0.02)\n",
        "\n",
        "    \"\"\"\n",
        "    to_srt(group_word_timestamps(res, combine_compound=combine_compound, ts_key='word_timestamps', min_dur=min_dur),\n",
        "           srt_path, strip=strip)\n",
        "\n",
        "\n",
        "def _get_min_estimation(estimations: List[Union[list, np.ndarray]],\n",
        "                        min_: (int, float) = None,\n",
        "                        max_: (int, float) = None) -> np.ndarray:\n",
        "    estimations = deepcopy(estimations)\n",
        "    estimations = list(map(lambda est_: np.array(est_) if isinstance(est_, list) else est_, estimations))\n",
        "    prev_min = min_ or 0\n",
        "    curr_max = max_ or np.max(estimations[-1])\n",
        "\n",
        "    min_est = []\n",
        "    for curr_est in estimations:\n",
        "        curr_min = curr_est[np.logical_and(curr_max > curr_est, curr_est > prev_min)]\n",
        "        curr_min = np.min(curr_min) if curr_min.shape[0] else prev_min\n",
        "        min_est.append(curr_min)\n",
        "        prev_min = curr_min\n",
        "\n",
        "    return np.array(min_est)\n",
        "\n",
        "\n",
        "def _get_max_estimation(estimations: List[Union[list, np.ndarray]],\n",
        "                        max_: (int, float) = None,\n",
        "                        min_: (int, float) = None) -> np.ndarray:\n",
        "    estimations = deepcopy(estimations)\n",
        "    estimations = list(map(lambda est_: np.array(est_) if isinstance(est_, list) else est_, estimations))\n",
        "    prev_max = max_ or np.max(estimations[-1])\n",
        "    curr_min = np.min(estimations[0]) if min_ is None else min_\n",
        "\n",
        "    max_est = []\n",
        "    for curr_est in reversed(estimations):\n",
        "        curr_max = curr_est[np.logical_and(prev_max > curr_est, curr_est > curr_min)]\n",
        "        curr_max = np.max(curr_max) if curr_max.shape[0] else prev_max\n",
        "        max_est.append(curr_max)\n",
        "        prev_max = curr_max\n",
        "\n",
        "    max_est.reverse()\n",
        "    return np.array(max_est)\n",
        "\n",
        "\n",
        "def _remove_overestimation(x: Union[np.ndarray, List[Union[int, float]]], alt_est: List[Union[list, np.ndarray]] = None,\n",
        "                           max_: (int, float) = None, min_: (int, float) = None,\n",
        "                           aggressive=False) -> np.ndarray:\n",
        "    x = np.array(x) if isinstance(x, list) else deepcopy(x)\n",
        "    if alt_est is not None:\n",
        "        alt_est = list(map(lambda est_: np.array(est_) if isinstance(est_, list) else est_, alt_est))\n",
        "    assert x.ndim == 1\n",
        "    assert alt_est is None or len(alt_est) == x.shape[0]\n",
        "    max_val = x[-1] if max_ is None else max_\n",
        "    min_val = x[0] if min_ is None else min_\n",
        "\n",
        "    def curr_max_min(val):\n",
        "        if min_ is None:\n",
        "            return val\n",
        "        return max(min_, val)\n",
        "\n",
        "    if min_ is not None:\n",
        "        x[x < min_] = min_\n",
        "    reduce_ = np.min if aggressive else np.mean\n",
        "    for i in range(x.shape[-1] - 1, -1, -1):\n",
        "        if x[i] > max_val or (i > 1 and x[i] < reduce_(x[:i])):  # spikes or dips\n",
        "            if alt_est is None or alt_est[i] is None:\n",
        "                x[i] = max_val\n",
        "            else:\n",
        "                tmp_min = min_val if i < 2 else curr_max_min(np.mean(x[:i]))\n",
        "                alt_ = alt_est[i][np.logical_and(alt_est[i] < max_val, alt_est[i] > tmp_min)]\n",
        "                x[i] = max_val if alt_.shape[0] == 0 else alt_[0]\n",
        "        max_val = x[i]\n",
        "    return x\n",
        "\n",
        "\n",
        "def _remove_underestimation(x: Union[np.ndarray, List[Union[int, float]]],\n",
        "                            alt_est: List[Union[list, np.ndarray]] = None,\n",
        "                            min_: (int, float) = None, max_: (int, float) = None,\n",
        "                            aggressive=False) -> np.ndarray:\n",
        "    x = np.array(x) if isinstance(x, list) else deepcopy(x)\n",
        "    if alt_est is not None:\n",
        "        alt_est = list(map(lambda est_: np.array(est_) if isinstance(est_, list) else est_, alt_est))\n",
        "    assert x.ndim == 1\n",
        "    assert alt_est is None or len(alt_est) == x.shape[0]\n",
        "    min_val = x[0] if min_ is None else min_\n",
        "    max_val = x[-1] if max_ is None else max_\n",
        "\n",
        "    def curr_min_max(val):\n",
        "        if max_ is None:\n",
        "            return val\n",
        "        return min(max_, val)\n",
        "\n",
        "    if max_ is not None:\n",
        "        x[x > max_] = max_\n",
        "    reduce_ = np.max if aggressive else np.mean\n",
        "    max_i_reduce = x.shape[-1] - 2\n",
        "    for i in range(0, x.shape[-1]):\n",
        "        if x[i] < min_val or (i < max_i_reduce and x[i] > reduce_(x[i + 1:])):  # dips or spikes\n",
        "            if alt_est is None or alt_est[i] is None:\n",
        "                x[i] = min_val\n",
        "            else:\n",
        "                tmp_max = max_val if i >= max_i_reduce else curr_min_max(np.mean(x[i + 1:]))\n",
        "                alt_ = alt_est[i][np.logical_and(alt_est[i] > min_val, alt_est[i] < tmp_max)]\n",
        "                x[i] = min_val if alt_.shape[0] == 0 else alt_[0]\n",
        "        min_val = x[i]\n",
        "    return x\n",
        "\n",
        "\n",
        "def _merge_max_min_estimation(mx: Union[np.ndarray, List[Union[int, float]]],\n",
        "                              mn: Union[np.ndarray, List[Union[int, float]]],\n",
        "                              alt_est: List[Union[list, np.ndarray]] = None) -> np.ndarray:\n",
        "    mx = np.array(mx) if isinstance(mx, list) else deepcopy(mx)\n",
        "    mn = np.array(mn) if isinstance(mn, list) else deepcopy(mn)\n",
        "    if alt_est is not None:\n",
        "        alt_est = list(map(lambda est_: np.array(est_) if isinstance(est_, list) else est_, alt_est))\n",
        "    assert mx.ndim == 1 and mn.ndim == 1\n",
        "    assert mx.shape[0] == mn.shape[0]\n",
        "    assert alt_est is None or len(alt_est) == mx.shape[0]\n",
        "\n",
        "    pref_mx = np.var(mx) > np.var(mn)\n",
        "    if pref_mx:\n",
        "        mn[0] = mx[0]\n",
        "    prev_min = mn[0]\n",
        "    for i in range(1, mn.shape[0]):\n",
        "        if prev_min > mn[i]:\n",
        "            if mn[i] > mx[i]:  # prev_min > mn[i] > mx[i]\n",
        "                mn[i] = prev_min\n",
        "            elif mx[i] > mn[i]:\n",
        "                if prev_min > mx[i]:  # prev_min > mx[i] > mn[i]\n",
        "                    mn[i] = prev_min\n",
        "                else:  # mx[i] > prev_min > mn[i]\n",
        "                    alt_ = alt_est[i][np.logical_and(alt_est[i] > prev_min, alt_est[i] < mx[i])]\n",
        "                    mn[i] = (mx[i] if pref_mx else prev_min) if alt_.shape[0] == 0 else alt_[0]\n",
        "            else:  # prev_min > mn[i] == mx[i]\n",
        "                mn[i] = prev_min\n",
        "        elif mn[i] > prev_min:\n",
        "            # if prev_min > mx[i]:  # mn[i] > prev_min > mx[i]\n",
        "            #     pass\n",
        "            if mx[i] > prev_min:\n",
        "                if mn[i] > mx[i]:  # mn[i] > mx[i] > prev_min\n",
        "                    pass\n",
        "                elif mx[i] > mn[i]:  # mx[i] > mn[i] > prev_min\n",
        "                    alt_ = alt_est[i][np.logical_and(alt_est[i] > mn[i], alt_est[i] < mx[i])]\n",
        "                    if alt_.shape[0]:\n",
        "                        mn[i] = alt_[0]\n",
        "                    elif pref_mx:\n",
        "                        mn[i] = mx[i]\n",
        "            #     else:  # mx[i] == mn[i] > prev_min\n",
        "            #         pass\n",
        "            # else:  # mn[i] > mx[i] == prev_min\n",
        "            #     pass\n",
        "        else:  # mn[i] == prev_min\n",
        "            if mx[i] > mn[i]:  # mx[i] > mn[i] == prev_min\n",
        "                alt_ = alt_est[i][np.logical_and(alt_est[i] > mn[i], alt_est[i] < mx[i])]\n",
        "                if alt_.shape[0]:\n",
        "                    mn[i] = alt_[0]\n",
        "                elif pref_mx:\n",
        "                    mn[i] = mx[i]\n",
        "            # elif mn[i] > mx[i]:  # mn[i] == prev_min > mx[i]\n",
        "            #     pass\n",
        "            # else:  # mn[i] == prev_min == mx[i]\n",
        "            #     pass\n",
        "\n",
        "        prev_min = mn[i]\n",
        "\n",
        "    return mn\n",
        "\n",
        "\n",
        "def _avg_merge_min_max(mx: Union[np.ndarray, List[Union[int, float]]],\n",
        "                       mn: Union[np.ndarray, List[Union[int, float]]],\n",
        "                       alt_timestamps: List[Union[List[Union[int, float]], np.ndarray]] = None,\n",
        "                       max_: (int, float) = None, min_: (int, float) = None):\n",
        "    mx = np.array(mx) if isinstance(mx, list) else deepcopy(mx)\n",
        "    mn = np.array(mn) if isinstance(mn, list) else deepcopy(mn)\n",
        "    assert mx.ndim == mn.ndim == 1\n",
        "    assert mx.shape[0] == mn.shape[0]\n",
        "\n",
        "    avg_ = (mx + mn) / 2\n",
        "\n",
        "    if check_ascending_sequence(avg_, verbose=False):\n",
        "        return avg_\n",
        "\n",
        "    if not max_:\n",
        "        max_ = max(mx[-1], mn[-1])\n",
        "    if min_ is None:\n",
        "        min_ = min(mn[0], mx[0])\n",
        "\n",
        "    return _stabilize_timestamps(avg_, alt_timestamps, max_=max_, min_=min_)\n",
        "\n",
        "\n",
        "def _stabilize_timestamps(timestamps: Union[np.ndarray, List[Union[int, float]]],\n",
        "                          alt_timestamps: List[Union[List[Union[int, float]], np.ndarray]] = None,\n",
        "                          max_: (int, float) = None, min_: (int, float) = None, aggressive=False) -> np.ndarray:\n",
        "    mx = _remove_overestimation(timestamps, alt_est=alt_timestamps, max_=max_, min_=min_, aggressive=aggressive)\n",
        "    mn = _remove_underestimation(timestamps, alt_est=alt_timestamps, max_=max_, min_=min_, aggressive=aggressive)\n",
        "    return _merge_max_min_estimation(mx, mn, alt_timestamps)\n",
        "\n",
        "\n",
        "def _stabilize_more_timestamps(timestamps: List[Union[list, np.ndarray]],\n",
        "                               max_: (int, float) = None, min_: (int, float) = None, average=True) -> np.ndarray:\n",
        "    mx = _get_max_estimation(timestamps, max_=max_, min_=min_)\n",
        "    mn = _get_min_estimation(timestamps, max_=max_, min_=min_)\n",
        "    if average:\n",
        "        return _avg_merge_min_max(mx, mn, timestamps, max_=max_, min_=min_)\n",
        "    return _merge_max_min_estimation(mx, mn, timestamps)\n",
        "\n",
        "\n",
        "def stabilize_timestamps(segments: Union[List[dict], dict],\n",
        "                         top_focus=False, aggressive=False, average=True) -> List[dict]:\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    segments: Union[List[dict], dict]\n",
        "        result['segments'] or result\n",
        "    top_focus: bool\n",
        "        adhere closely to the top predictions for word timestamps\n",
        "    aggressive: bool\n",
        "        only if top_focus=True,\n",
        "        allow greater variation in word_timestamps/whole_word_timestamps\n",
        "    average: bool\n",
        "        only if top_focus=False,\n",
        "        average min and max of unstable_word_timestamps to get word_timestamps/whole_word_timestamps\n",
        "\n",
        "    \"\"\"\n",
        "    if isinstance(segments, dict):\n",
        "        segments = segments['segments']\n",
        "    if not segments:\n",
        "        warnings.warn('No Segments Found')\n",
        "        return []\n",
        "    missing_ts_idx = set(map(lambda x: None if x[1].get('unstable_word_timestamps') else x[0], enumerate(segments))) - {\n",
        "        None}\n",
        "    no_word_timestamps = len(missing_ts_idx) == len(segments)\n",
        "    if not no_word_timestamps and missing_ts_idx:\n",
        "        warnings.warn(f'Segments {list(missing_ts_idx)} are missing unstable_word_timestamps. '\n",
        "                      f'Word-level timestamp stabilization will skipped')\n",
        "\n",
        "    segments = deepcopy(segments)\n",
        "    sectioned_segments: List[List] = [[]]\n",
        "    for i, seg in enumerate(segments, 1):\n",
        "        sectioned_segments[-1].append(seg)\n",
        "        if seg['anchor_point']:\n",
        "            if i < len(segments):\n",
        "                sectioned_segments.append([])\n",
        "\n",
        "    assert all(set(len(set(s['offset'] for s in segs)) == 1 for segs in sectioned_segments))\n",
        "\n",
        "    sectioned_segments_timestamps = [dict(min_=segs[-1]['offset'],\n",
        "                                          max_=segs[-1]['next_offset'],\n",
        "                                          timestamps=list(chain.from_iterable((s['start'], s['end']) for s in segs)),\n",
        "                                          alt_timestamps=list(chain.from_iterable((s['alt_start_timestamps'],\n",
        "                                                                                   s['alt_end_timestamps'])\n",
        "                                                                                  for s in segs)))\n",
        "                                     for segs in sectioned_segments]\n",
        "\n",
        "    sectioned_stab_timestamps = [_stabilize_timestamps(**kwargs).reshape(-1, 2) for kwargs in\n",
        "                                 sectioned_segments_timestamps]\n",
        "\n",
        "    for i in range(len(sectioned_segments)):\n",
        "        for j in range(len(sectioned_segments[i])):\n",
        "            sectioned_segments[i][j]['start'], sectioned_segments[i][j]['end'] = sectioned_stab_timestamps[i][j]\n",
        "\n",
        "            if not missing_ts_idx:\n",
        "                if top_focus:\n",
        "                    top_word_ts = [ts_['timestamps'][0] for ts_ in\n",
        "                                   sectioned_segments[i][j]['unstable_word_timestamps']]\n",
        "                    alt_word_ts = [ts_['timestamps'][1:] for ts_ in\n",
        "                                   sectioned_segments[i][j]['unstable_word_timestamps']]\n",
        "                    temp_stab_word_ts = _stabilize_timestamps(top_word_ts, alt_word_ts,\n",
        "                                                              max_=sectioned_segments[i][j]['end'],\n",
        "                                                              min_=sectioned_segments[i][j]['start'],\n",
        "                                                              aggressive=aggressive)\n",
        "                else:\n",
        "                    word_ts = [ts_['timestamps'] for ts_ in sectioned_segments[i][j]['unstable_word_timestamps']]\n",
        "                    temp_stab_word_ts = _stabilize_more_timestamps(word_ts,\n",
        "                                                                   max_=sectioned_segments[i][j]['end'],\n",
        "                                                                   min_=sectioned_segments[i][j]['start'],\n",
        "                                                                   average=average)\n",
        "\n",
        "                temp_stab_word_ts = [{'word': sectioned_segments[i][j]['unstable_word_timestamps'][k]['word'],\n",
        "                                      'token': sectioned_segments[i][j]['unstable_word_timestamps'][k]['token'],\n",
        "                                      'timestamp': temp_stab_word_ts[k]}\n",
        "                                     for k in range(temp_stab_word_ts.shape[0])]\n",
        "\n",
        "                sectioned_segments[i][j]['word_timestamps'] = temp_stab_word_ts\n",
        "\n",
        "    return list(chain.from_iterable(sectioned_segments))\n",
        "\n",
        "\n",
        "def save_as_json(results, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "\n",
        "def add_whole_word_ts(tokenizer: Tokenizer, segments: Union[List[dict], dict], merge_non_space: bool = None,\n",
        "                      prepend_punctuations: Union[List[str], Tuple[str]] = None,\n",
        "                      append_punctuations: Union[List[str], Tuple[str]] = None):\n",
        "    merge_non_space = (tokenizer.language in ['en'] or tokenizer.language is None) \\\n",
        "        if merge_non_space is None else merge_non_space\n",
        "    if prepend_punctuations is None:\n",
        "        prepend_punctuations = r'“¿([{'\n",
        "    if append_punctuations is None:\n",
        "        append_punctuations = r'.。,，!！?？:：”)]}、'\n",
        "    if isinstance(segments, dict):\n",
        "        segments = segments['segments']\n",
        "    if not segments:\n",
        "        print('No segments found, whole-word timestamps cannot be added.')\n",
        "        return\n",
        "\n",
        "    missing_idx = set(-1 if seg.get('word_timestamps') else i for i, seg in enumerate(segments)) - {-1}\n",
        "\n",
        "    if missing_idx:\n",
        "        if len(missing_idx) == len(segments):\n",
        "            print('No word_timestamps found, whole-word timestamps cannot be added.')\n",
        "            return\n",
        "        print(f'Some word_timestamps not found, '\n",
        "              f'whole-word timestamps cannot be added to the following segments: {tuple(missing_idx)}')\n",
        "\n",
        "    failed_idx = []\n",
        "\n",
        "    for seg_idx, seg in enumerate(segments):\n",
        "        if seg.get('word_timestamps'):\n",
        "            prev_idx = 0\n",
        "            remaining_text = seg['text']\n",
        "            has_prepend = False\n",
        "            whole_word_timestamps: List[dict] = []\n",
        "            for wts_idx in range(1, len(seg['word_timestamps']) + 1):\n",
        "                max_ts = seg['word_timestamps'][wts_idx - 1]['timestamp']\n",
        "                tokens = [wts['token'] for wts in seg['word_timestamps'][prev_idx: wts_idx]]\n",
        "                temp_whole_word = tokenizer.decode(tokens)\n",
        "                if temp_whole_word == remaining_text[:len(temp_whole_word)]:\n",
        "                    prev_idx = wts_idx\n",
        "                    remaining_text = remaining_text[len(temp_whole_word):]\n",
        "                    if (not merge_non_space or temp_whole_word.startswith(' ') or not whole_word_timestamps) and \\\n",
        "                            temp_whole_word not in append_punctuations and \\\n",
        "                            not has_prepend:\n",
        "                        has_prepend = temp_whole_word.strip() in prepend_punctuations\n",
        "                        whole_word_timestamps.append(dict(word=temp_whole_word, timestamp=max_ts))\n",
        "                    else:\n",
        "                        has_prepend = False\n",
        "                        whole_word_timestamps[-1]['word'] += temp_whole_word\n",
        "                        whole_word_timestamps[-1]['timestamp'] = max_ts\n",
        "            if remaining_text:\n",
        "                failed_idx.append(seg_idx)\n",
        "                whole_word_timestamps = []\n",
        "            seg['whole_word_timestamps'] = whole_word_timestamps or None\n",
        "        else:\n",
        "            seg['whole_word_timestamps'] = None\n",
        "\n",
        "    if failed_idx:\n",
        "        print(f'Failed to add whole-word timestamps to the following segments: {tuple(failed_idx)}')\n",
        "\n",
        "\n",
        "def _load_audio_waveform(audio: Union[str, bytes, np.ndarray, torch.Tensor], h: int, w: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    audio: Union[str, bytes, np.ndarray, torch.Tensor], shape = (*)\n",
        "        The path to audio or bytes of audio file or a NumPy array or Tensor containing the audio waveform in 16 kHz\n",
        "    h: int\n",
        "        Height of waveform image\n",
        "    w: int\n",
        "        Width of waveform image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Audio waveform image as a NumPy array, in uint8 dtype.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        if isinstance(audio, str):\n",
        "            stream = ffmpeg.input(audio, threads=0)\n",
        "            inp = None\n",
        "\n",
        "        else:\n",
        "            if isinstance(audio, bytes):\n",
        "                stream = ffmpeg.input('pipe:', threads=0)\n",
        "                inp = audio\n",
        "            else:\n",
        "                warnings.warn('A resampled input causes an unexplained temporal shift in waveform image '\n",
        "                              'that will skew the timestamp suppression and may result in inaccurate timestamps.\\n'\n",
        "                              'Use audio_for_mask for transcribe() to provide the original audio track '\n",
        "                              'as the path or bytes of the audio file.',\n",
        "                              stacklevel=2)\n",
        "                stream = ffmpeg.input('pipe:', threads=0, ac=1, format='s16le')\n",
        "                if isinstance(audio, torch.Tensor):\n",
        "                    audio = np.array(audio)\n",
        "                inp = (audio * 32768.0).astype(np.int16).tobytes()\n",
        "\n",
        "        waveform, err = (\n",
        "            stream.filter('aformat', channel_layouts='mono')\n",
        "                .filter('highpass', f='200').filter('lowpass', f='3000')\n",
        "                .filter('showwavespic', s=f'{w}x{h}')\n",
        "                .output('-', pix_fmt='gray', format='rawvideo')\n",
        "                .run(cmd=\"ffmpeg\", capture_stdout=True, capture_stderr=True, input=inp)\n",
        "        )\n",
        "    except ffmpeg.Error as e:\n",
        "        raise RuntimeError(f\"Failed to load audio in waveform: {e.stderr.decode()}\") from e\n",
        "    else:\n",
        "        if not waveform:\n",
        "            partial_file = b'partial file' in err and b'Output file is empty' in err\n",
        "            add_msg = '\\nMetadata for decoding are likely at end of file, try to use path of audio instead.' \\\n",
        "                if partial_file and isinstance(audio, bytes) else ''\n",
        "            raise RuntimeError(f\"Failed to load audio in waveform: {err.decode()}\" + add_msg)\n",
        "        return np.frombuffer(waveform, dtype=np.uint8).reshape(h, w)\n",
        "\n",
        "\n",
        "def _remove_lower_quantile(waveform: np.ndarray,\n",
        "                           upper_quantile: float = None,\n",
        "                           lower_quantile: float = None,\n",
        "                           lower_threshold: float = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Removes lower quantile of amplitude from waveform image\n",
        "    \"\"\"\n",
        "    if upper_quantile is None:\n",
        "        upper_quantile = 0.85\n",
        "    if lower_quantile is None:\n",
        "        lower_quantile = 0.15\n",
        "    if lower_threshold is None:\n",
        "        lower_threshold = 0.15\n",
        "    waveform = deepcopy(waveform)\n",
        "    wave_sums = waveform.sum(0)\n",
        "    mx = np.quantile(wave_sums, upper_quantile, -1)\n",
        "    mn = np.quantile(wave_sums, lower_quantile, -1)\n",
        "    mn_threshold = (mx - mn) * lower_threshold + mn\n",
        "    waveform[:, wave_sums < mn_threshold] = 0\n",
        "    return waveform\n",
        "\n",
        "\n",
        "def _wave_to_ts_filter(waveform: np.ndarray, suppress_middle=True,\n",
        "                       max_index: (list, int) = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns A NumPy array mask of sections with amplitude zero\n",
        "    \"\"\"\n",
        "    assert waveform.ndim <= 2, f'waveform have at most 2 dims but found {waveform.ndim}'\n",
        "    if waveform.ndim == 1:\n",
        "        wave_sum = waveform\n",
        "    else:\n",
        "        wave_sum = waveform.sum(-2)\n",
        "\n",
        "    wave_filter = wave_sum.astype(bool)\n",
        "\n",
        "    if not suppress_middle:\n",
        "        nonzero_indices = wave_filter.nonzero()[0]\n",
        "        wave_filter[nonzero_indices[0]:nonzero_indices[-1] + 1] = True\n",
        "    if max_index is not None:\n",
        "        wave_filter[max_index + 1:] = False\n",
        "\n",
        "    return ~wave_filter\n",
        "\n",
        "\n",
        "# modified version of whisper.transcribe.transcribe\n",
        "def transcribe_word_level(\n",
        "        model: \"Whisper\",\n",
        "        audio: Union[str, np.ndarray, torch.Tensor],\n",
        "        *,\n",
        "        verbose: bool = False,\n",
        "        temperature: Union[float, Tuple[float, ...]] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
        "        compression_ratio_threshold: Optional[float] = 2.4,\n",
        "        logprob_threshold: Optional[float] = -1.0,\n",
        "        no_speech_threshold: Optional[float] = 0.6,\n",
        "        condition_on_previous_text: bool = True,\n",
        "        stab=True, top_focus=False, ts_num: int = 10,\n",
        "        alpha: float = None, print_unstab=False,\n",
        "        suppress_silence: bool = True,\n",
        "        suppress_middle: bool = True,\n",
        "        suppress_word_ts: bool = True,\n",
        "        remove_background: bool = True,\n",
        "        silence_threshold: float = 0.1,\n",
        "        prepend_punctuations: Union[List[str], Tuple[str]] = None,\n",
        "        append_punctuations: Union[List[str], Tuple[str]] = None,\n",
        "        audio_for_mask: (str, bytes) = None,\n",
        "        **decode_options):\n",
        "    \"\"\"\n",
        "    Transcribe an audio file using Whisper\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: Whisper\n",
        "        The Whisper model instance\n",
        "\n",
        "    audio: Union[str, np.ndarray, torch.Tensor]\n",
        "        The path to the audio file to open, or the audio waveform\n",
        "\n",
        "    verbose: bool\n",
        "        Whether to display the decoded text (with finalized timestamps) to the console\n",
        "\n",
        "    temperature: Union[float, Tuple[float, ...]]\n",
        "        Temperature for sampling. It can be a tuple of temperatures, which will be successfully used\n",
        "        upon failures according to either `compression_ratio_threshold` or `logprob_threshold`.\n",
        "\n",
        "    compression_ratio_threshold: float\n",
        "        If the gzip compression ratio is above this value, treat as failed\n",
        "\n",
        "    logprob_threshold: float\n",
        "        If the average log probability over sampled tokens is below this value, treat as failed\n",
        "\n",
        "    no_speech_threshold: float\n",
        "        If the no_speech probability is higher than this value AND the average log probability\n",
        "        over sampled tokens is below `logprob_threshold`, consider the segment as silent\n",
        "\n",
        "    condition_on_previous_text: bool\n",
        "        if True, the previous output of the model is provided as a prompt for the next window;\n",
        "        disabling may make the text inconsistent across windows, but the model becomes less prone to\n",
        "        getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n",
        "\n",
        "    stab: bool\n",
        "        Stabilizing timestamps by cross compare timestamps and using additional top timestamp predictions\n",
        "        to fill in when appropriate to ensure timestamps are chronological.\n",
        "\n",
        "    top_focus: bool\n",
        "        Adhere closely to the top predictions for token timestamps stabilization\n",
        "\n",
        "    ts_num: int\n",
        "        Number of top timestamp predictions to save for each word for postprocessing stabilization (default: 10).\n",
        "\n",
        "    alpha: float\n",
        "        Amount of noise to add to audio to produce slightly difference results.\n",
        "        audio_features *= torch.rand_like(audio_features) * alpha + 1\n",
        "\n",
        "    print_unstab: bool\n",
        "        Whether to display the text (without stabilize timestamps) being decoded to the console\n",
        "        (i.e. behaves like verbose before model was modified)\n",
        "\n",
        "    suppress_silence: bool\n",
        "        Suppress timestamp tokens that are marked as silent\n",
        "\n",
        "    suppress_middle: bool\n",
        "        Suppress any silent timestamps tokens of middle of the segment instead of only beginning and ending\n",
        "\n",
        "    suppress_word_ts: bool\n",
        "        Suppress timestamp tokens of words that are marked as silent\n",
        "\n",
        "    remove_background: bool\n",
        "        Whether to remove background noise from waveform so that it is marked silent.\n",
        "        Determined by parameters part of decode_options (i.e. specify like other options here):\n",
        "            upper_quantile: float\n",
        "                The upper quantile of amplitude to determine a max amplitude, mx (Default: 0.85)\n",
        "            lower_quantile: float\n",
        "                The lower quantile of amplitude to determine a min amplitude, mn (Default: 0.15)\n",
        "            lower_threshold: float\n",
        "                Suppressed sections of waveform where amplitude < lower_threshold*(mx-mn) + mn. (Default: 0.15)\n",
        "\n",
        "    silence_threshold: float:\n",
        "        Audio segments silence average >= silence_threshold\n",
        "        then that segment will not have background removed even if remove_background=True.\n",
        "        e.g. 0.5 means if less than half of the audio segment is silent then background will be removed accordingly\n",
        "\n",
        "    prepend_punctuations: Union[List[str], Tuple[str]]\n",
        "        Punctuations to prepend to next word (Default: “¿([{)\n",
        "\n",
        "    append_punctuations: Union[List[str], Tuple[str]]\n",
        "        Punctuations to append to previous word (Default: .。,，!！?？:：”)]}、)\n",
        "\n",
        "    audio_for_mask: (str, bytes)\n",
        "        Original audio track as path or bytes of audio file.\n",
        "        Since resampled audio may shift the waveform image,\n",
        "        this is an alternative to 'audio' option to generate suppression mask from the original audio.\n",
        "\n",
        "    decode_options: dict\n",
        "        Keyword arguments to construct `DecodingOptions` instances\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A dictionary containing the resulting text (\"text\") and segment-level details (\"segments\"), and\n",
        "    the spoken language (\"language\"), which is detected when `decode_options[\"language\"]` is None.\n",
        "    \"\"\"\n",
        "\n",
        "    if 'no_captions_threshold' in decode_options:\n",
        "        warnings.warn('no_captions_threshold is deprecated. '\n",
        "                      'Please use no_speech_threshold instead.', DeprecationWarning, stacklevel=2)\n",
        "        no_speech_threshold = decode_options.pop('no_captions_threshold')\n",
        "\n",
        "    dtype = torch.float16 if decode_options.get(\"fp16\", True) else torch.float32\n",
        "    if model.device == torch.device(\"cpu\"):\n",
        "        if torch.cuda.is_available():\n",
        "            warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
        "        if dtype == torch.float16:\n",
        "            warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
        "            dtype = torch.float32\n",
        "\n",
        "    if dtype == torch.float32:\n",
        "        decode_options[\"fp16\"] = False\n",
        "\n",
        "    if 'max_initial_timestamp' not in decode_options:\n",
        "        decode_options['max_initial_timestamp'] = None\n",
        "\n",
        "    mel = log_mel_spectrogram(audio)\n",
        "\n",
        "    if decode_options.get(\"language\", None) is None:\n",
        "        if verbose:\n",
        "            print(\"Detecting language using up to the first 30 seconds. Use `--language` to specify the language\")\n",
        "        segment = pad_or_trim(mel, N_FRAMES).to(model.device).to(dtype)\n",
        "        _, probs = model.detect_language(segment)\n",
        "        decode_options[\"language\"] = max(probs, key=probs.get)\n",
        "        print(f\"Detected language: {LANGUAGES[decode_options['language']]}\")\n",
        "\n",
        "    mel = mel.unsqueeze(0)\n",
        "    language = decode_options[\"language\"]\n",
        "    task = decode_options.get(\"task\", \"transcribe\")\n",
        "    tokenizer = get_tokenizer(model.is_multilingual, language=language, task=task)\n",
        "\n",
        "    def decode_with_fallback(segment: torch.Tensor, suppress_ts_mask: Tensor = None) \\\n",
        "            -> Union[List[DecodingResult], tuple]:\n",
        "        temperatures = [temperature] if isinstance(temperature, (int, float)) else temperature\n",
        "        kwargs = {**decode_options}\n",
        "        t = temperatures[0]\n",
        "        if t == 0:\n",
        "            best_of = kwargs.pop(\"best_of\", None)\n",
        "        else:\n",
        "            best_of = kwargs.get(\"best_of\", None)\n",
        "\n",
        "        options = DecodingOptions(**kwargs, temperature=t)\n",
        "        results, ts_tokens, ts_logits_ = model.decode(segment, options, ts_num=ts_num, alpha=alpha,\n",
        "                                                      suppress_ts_mask=suppress_ts_mask,\n",
        "                                                      suppress_word_ts=suppress_word_ts)\n",
        "\n",
        "        kwargs.pop(\"beam_size\", None)  # no beam search for t > 0\n",
        "        kwargs.pop(\"patience\", None)  # no patience for t > 0\n",
        "        kwargs[\"best_of\"] = best_of  # enable best_of for t > 0\n",
        "        for t in temperatures[1:]:\n",
        "            needs_fallback = [\n",
        "                compression_ratio_threshold is not None\n",
        "                and result.compression_ratio > compression_ratio_threshold\n",
        "                or logprob_threshold is not None\n",
        "                and result.avg_logprob < logprob_threshold\n",
        "                for result in results\n",
        "            ]\n",
        "            if any(needs_fallback):\n",
        "                options = DecodingOptions(**kwargs, temperature=t)\n",
        "                retries, r_ts_tokens, r_ts_logits = model.decode(segment[needs_fallback], options,\n",
        "                                                                 ts_num=ts_num, alpha=alpha,\n",
        "                                                                 suppress_ts_mask=suppress_ts_mask,\n",
        "                                                                 suppress_word_ts=suppress_word_ts)\n",
        "                for retry_index, original_index in enumerate(np.nonzero(needs_fallback)[0]):\n",
        "                    results[original_index] = retries[retry_index]\n",
        "                    ts_tokens[original_index] = r_ts_tokens[retry_index]\n",
        "                    ts_logits_[original_index] = r_ts_logits[retry_index]\n",
        "\n",
        "        return results, ts_tokens, ts_logits_\n",
        "\n",
        "    seek = 0\n",
        "    input_stride = exact_div(\n",
        "        N_FRAMES, model.dims.n_audio_ctx\n",
        "    )  # mel frames per output token: 2\n",
        "    time_precision = (\n",
        "            input_stride * HOP_LENGTH / SAMPLE_RATE\n",
        "    )  # time per output token: 0.02 (seconds)\n",
        "    all_tokens = []\n",
        "    all_segments = []\n",
        "    prompt_reset_since = 0\n",
        "\n",
        "    initial_prompt = decode_options.pop(\"initial_prompt\", None) or []\n",
        "    if initial_prompt:\n",
        "        initial_prompt = tokenizer.encode(\" \" + initial_prompt.strip())\n",
        "        all_tokens.extend(initial_prompt)\n",
        "\n",
        "    def _to_list(x: (Tensor, None)):\n",
        "        if x is None:\n",
        "            return x\n",
        "        return x.tolist()\n",
        "\n",
        "    def add_segment(\n",
        "            *, offset: float, start: float, end: float, text_tokens: Tensor, result: DecodingResult,\n",
        "            start_timestamps: list = None, end_timestamps: list = None, word_timestamps: Tensor = None,\n",
        "            start_ts_logits: list = None, end_ts_logits: list = None, word_ts_logits: Tensor = None\n",
        "    ):\n",
        "        no_eot_mask = text_tokens < tokenizer.eot\n",
        "        text_tokens_no_eot = text_tokens[no_eot_mask]\n",
        "        text = tokenizer.decode(text_tokens_no_eot)\n",
        "\n",
        "        if len(text.strip()) == 0:  # skip empty text output\n",
        "            return\n",
        "\n",
        "        if word_timestamps is not None:\n",
        "            assert word_timestamps.shape[0] == text_tokens.shape[0]\n",
        "            if word_ts_logits is None:\n",
        "                word_ts_fields = zip(text_tokens_no_eot, word_timestamps[no_eot_mask], repeat(None))\n",
        "            else:\n",
        "                assert word_ts_logits.shape[0] == text_tokens.shape[0]\n",
        "                word_ts_fields = zip(text_tokens_no_eot, word_timestamps[no_eot_mask], word_ts_logits[no_eot_mask])\n",
        "\n",
        "            word_timestamps = [dict(word=tokenizer.decode([token]),\n",
        "                                    token=token.item(),\n",
        "                                    timestamps=timestamps_.tolist(),\n",
        "                                    timestamp_logits=_to_list(ts_logits_))\n",
        "                               for token, timestamps_, ts_logits_ in word_ts_fields]\n",
        "\n",
        "        all_segments.append(\n",
        "            {\n",
        "                \"id\": len(all_segments),\n",
        "                \"seek\": seek,\n",
        "                'offset': offset,  # offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "                \"text\": text,\n",
        "                \"tokens\": result.tokens,\n",
        "                \"temperature\": result.temperature,\n",
        "                \"avg_logprob\": result.avg_logprob,\n",
        "                \"compression_ratio\": result.compression_ratio,\n",
        "                \"no_speech_prob\": get_new_attrs(result, 'no_caption_prob'),\n",
        "                \"alt_start_timestamps\": start_timestamps,\n",
        "                \"start_ts_logits\": start_ts_logits,\n",
        "                \"alt_end_timestamps\": end_timestamps,\n",
        "                \"end_ts_logits\": end_ts_logits,\n",
        "                \"unstable_word_timestamps\": word_timestamps,\n",
        "                'anchor_point': False\n",
        "            }\n",
        "        )\n",
        "        if print_unstab or (verbose and not stab):\n",
        "            print(f'[{format_timestamp(start)} --> {format_timestamp(end)}] \"{text}\"')\n",
        "            if word_timestamps is not None:\n",
        "                ts_str = (f' ->[{format_timestamp(ts_[\"timestamps\"][0])}] \"{ts_[\"word\"].strip()}\"' for ts_ in\n",
        "                          word_timestamps)\n",
        "                print('\\n'.join(ts_str), end='\\n\\n')\n",
        "\n",
        "    if suppress_silence:\n",
        "        ts_scale = HOP_LENGTH / SAMPLE_RATE / time_precision\n",
        "        wf = _load_audio_waveform(audio_for_mask or audio, 100, int(mel.shape[-1] * ts_scale))\n",
        "\n",
        "    upper_quantile = decode_options.pop('upper_quantile', 0.85)\n",
        "    lower_quantile = decode_options.pop('lower_quantile', 0.15)\n",
        "    lower_threshold = decode_options.pop('lower_threshold', 0.15)\n",
        "\n",
        "    while seek < mel.shape[-1]:\n",
        "        timestamp_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
        "        remaining_duration = float((mel.shape[-1] - seek) * HOP_LENGTH / SAMPLE_RATE)\n",
        "        segment = pad_or_trim(mel[:, :, seek:], N_FRAMES).to(model.device).to(dtype)\n",
        "        segment_duration = min(float(segment.shape[-1] * HOP_LENGTH / SAMPLE_RATE), remaining_duration)\n",
        "        segment_max_ts = segment_duration / time_precision\n",
        "\n",
        "        if suppress_silence:\n",
        "            wf_seek = int(seek * ts_scale)\n",
        "            segment_wf = wf[..., wf_seek:wf_seek + 1501]\n",
        "            if remove_background and \\\n",
        "                    (1 - segment_wf.sum(0).clip(max=1).mean()) < silence_threshold:\n",
        "                segment_wf = _remove_lower_quantile(segment_wf.astype(np.float32),\n",
        "                                                    upper_quantile=upper_quantile,\n",
        "                                                    lower_quantile=lower_quantile,\n",
        "                                                    lower_threshold=lower_threshold)\n",
        "            segment_wf = pad_or_trim(segment_wf, 1501)\n",
        "            suppress_ts_mask = torch.from_numpy(_wave_to_ts_filter(segment_wf,\n",
        "                                                                   suppress_middle=suppress_middle,\n",
        "                                                                   max_index=int(segment_max_ts)))\n",
        "\n",
        "            if suppress_ts_mask.all():  # segment is silent\n",
        "                seek += segment.shape[-1]  # fast-forward to the next segment boundary\n",
        "                continue\n",
        "        else:\n",
        "            suppress_ts_mask = None\n",
        "\n",
        "        decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n",
        "        result, finalized_ts_tokens, ts_logits = decode_with_fallback(segment,\n",
        "                                                                      suppress_ts_mask=suppress_ts_mask)\n",
        "\n",
        "        result = result[0]\n",
        "        tokens = torch.tensor(result.tokens)\n",
        "        finalized_ts_tokens = torch.tensor(finalized_ts_tokens[0])\n",
        "        ts_logits = torch.tensor(ts_logits[0])\n",
        "\n",
        "        if no_speech_threshold is not None:\n",
        "            # no voice activity check\n",
        "            should_skip = get_new_attrs(result, 'no_caption_prob') > no_speech_threshold\n",
        "            if logprob_threshold is not None and result.avg_logprob > logprob_threshold:\n",
        "                # don't skip if the logprob is high enough, despite the no_speech_prob\n",
        "                should_skip = False\n",
        "\n",
        "            if should_skip:\n",
        "                seek += segment.shape[-1]  # fast-forward to the next segment boundary\n",
        "                continue\n",
        "\n",
        "        timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n",
        "        consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0].add_(1)\n",
        "        if len(consecutive) > 0:  # if the output contains two consecutive timestamp tokens\n",
        "            last_slice = 0\n",
        "            for current_slice in consecutive:\n",
        "                sliced_tokens = tokens[last_slice:current_slice]\n",
        "                sliced_ts_tokens = finalized_ts_tokens[last_slice:current_slice]\n",
        "                sliced_ts_logits = ts_logits[last_slice:current_slice]\n",
        "                start_timestamp_position = (\n",
        "                        sliced_tokens[0].item() - tokenizer.timestamp_begin\n",
        "                )\n",
        "                end_timestamp_position = (\n",
        "                        sliced_tokens[-1].item() - tokenizer.timestamp_begin\n",
        "                )\n",
        "\n",
        "                word_ts = timestamp_offset + sliced_ts_tokens * time_precision\n",
        "\n",
        "                add_segment(\n",
        "                    offset=timestamp_offset,\n",
        "                    start=timestamp_offset + start_timestamp_position * time_precision,\n",
        "                    end=min(timestamp_offset + end_timestamp_position * time_precision,\n",
        "                            timestamp_offset + segment_duration),\n",
        "                    text_tokens=sliced_tokens[1:-1],\n",
        "                    result=result,\n",
        "                    start_timestamps=word_ts[0].tolist(),\n",
        "                    end_timestamps=word_ts[-1].tolist(),\n",
        "                    word_timestamps=word_ts[1:-1],\n",
        "                    start_ts_logits=sliced_ts_logits[0].tolist(),\n",
        "                    end_ts_logits=sliced_ts_logits[-1].tolist(),\n",
        "                    word_ts_logits=sliced_ts_logits[1:-1]\n",
        "                )\n",
        "                last_slice = current_slice\n",
        "            last_timestamp_position = (\n",
        "                min(tokens[last_slice - 1].item() - tokenizer.timestamp_begin, segment_max_ts)\n",
        "            )\n",
        "            seek += last_timestamp_position * input_stride\n",
        "            all_tokens.extend(tokens[: last_slice + 1].tolist())\n",
        "        else:\n",
        "            duration = segment_duration\n",
        "            timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n",
        "            if len(timestamps) > 0:\n",
        "                # no consecutive timestamps but it has a timestamp; use the last one.\n",
        "                # single timestamp at the end means no speech after the last timestamp.\n",
        "                last_timestamp_position = min(timestamps[-1].item() - tokenizer.timestamp_begin, segment_max_ts)\n",
        "                duration = last_timestamp_position * time_precision\n",
        "\n",
        "            word_ts = timestamp_offset + finalized_ts_tokens * time_precision\n",
        "\n",
        "            add_segment(\n",
        "                offset=timestamp_offset,\n",
        "                start=timestamp_offset,\n",
        "                end=timestamp_offset + duration,\n",
        "                text_tokens=tokens,\n",
        "                result=result,\n",
        "                word_timestamps=word_ts,\n",
        "                word_ts_logits=ts_logits\n",
        "            )\n",
        "\n",
        "            seek += segment.shape[-1]\n",
        "            all_tokens.extend(tokens.tolist())\n",
        "\n",
        "        if all_segments:\n",
        "            all_segments[-1]['anchor_point'] = True\n",
        "            all_segments[-1]['next_offset'] = float(seek * HOP_LENGTH / SAMPLE_RATE)\n",
        "        if not condition_on_previous_text or result.temperature > 0.5:\n",
        "            # do not feed the prompt tokens if a high temperature was used\n",
        "            prompt_reset_since = len(all_tokens)\n",
        "\n",
        "    if len(all_segments) > 1 and all_segments[-1]['alt_start_timestamps'] is None:\n",
        "        all_segments[-1]['alt_start_timestamps'] = all_segments[-2]['alt_end_timestamps']\n",
        "\n",
        "    if stab:\n",
        "        all_segments = stabilize_timestamps(all_segments, top_focus=top_focus)\n",
        "        add_whole_word_ts(tokenizer, all_segments,\n",
        "                          prepend_punctuations=prepend_punctuations,\n",
        "                          append_punctuations=append_punctuations)\n",
        "        if verbose:\n",
        "            print('\\nSTABILIZED\\n')\n",
        "            for seg_ in all_segments:\n",
        "                print(f'[{format_timestamp(seg_[\"start\"])} --> {format_timestamp(seg_[\"end\"])}] \"{seg_[\"text\"]}\"')\n",
        "                if seg_['word_timestamps']:\n",
        "                    ts_str = (f' ->[{format_timestamp(ts_[\"timestamp\"])}] \"{ts_[\"word\"].strip()}\"' for ts_ in\n",
        "                              seg_['word_timestamps'])\n",
        "                    print('\\n'.join(ts_str), end='\\n\\n')\n",
        "\n",
        "    return dict(text=tokenizer.decode(all_tokens[len(initial_prompt):]), segments=all_segments, language=language)\n",
        "\n",
        "\n",
        "def _suppress_ts(ts_logits: Tensor, suppress_ts_mask: Tensor = None):\n",
        "    if suppress_ts_mask is not None:\n",
        "        ts_logits[:, suppress_ts_mask] = -np.inf\n",
        "\n",
        "\n",
        "def _ts_topk(ts_logits: Tensor, k: int, prev_ts: Tensor = None) -> Tensor:\n",
        "    temp_ts = torch.stack(torch.topk(ts_logits, k, dim=-1), 0).unsqueeze(-2)\n",
        "    return temp_ts if prev_ts is None else torch.cat([prev_ts, temp_ts], dim=-2)\n",
        "\n",
        "\n",
        "# modified version of whisper.GreedyDecoder\n",
        "class GreedyDecoderWordLevel(GreedyDecoder):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.ts_num: int = kwargs.pop('ts_num', 10)\n",
        "        self.suppress_ts_mask: Tensor = kwargs.pop('suppress_ts_mask', None)\n",
        "        self.timestamp_begin = kwargs.pop('timestamp_begin', 50364)\n",
        "        super(GreedyDecoderWordLevel, self).__init__(*args, **kwargs)\n",
        "        self.ts = None\n",
        "\n",
        "    def _suppress_ts(self, logits: Tensor):\n",
        "        _suppress_ts(logits[:, self.timestamp_begin:],\n",
        "                     suppress_ts_mask=self.suppress_ts_mask)\n",
        "\n",
        "    def update_with_ts(self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor, ts: Tensor) -> Tuple[Tensor, bool]:\n",
        "        self.ts = ts\n",
        "\n",
        "        self._suppress_ts(logits)\n",
        "\n",
        "        if self.temperature == 0:\n",
        "            next_tokens = logits.argmax(dim=-1)\n",
        "        else:\n",
        "            next_tokens = Categorical(logits=logits / self.temperature).sample()\n",
        "\n",
        "        logprobs = F.log_softmax(logits.float(), dim=-1)\n",
        "        current_logprobs = logprobs[torch.arange(logprobs.shape[0]), next_tokens]\n",
        "        sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)\n",
        "\n",
        "        next_tokens[tokens[:, -1] == self.eot] = self.eot\n",
        "        tokens = torch.cat([tokens, next_tokens[:, None]], dim=-1)\n",
        "\n",
        "        completed = (tokens[:, -1] == self.eot).all()\n",
        "        return tokens, completed\n",
        "\n",
        "    def finalize(self, tokens: Tensor, sum_logprobs: Tensor):\n",
        "        # make sure each sequence has at least one EOT token at the end\n",
        "        tokens = F.pad(tokens, (0, 1), value=self.eot)\n",
        "        return tokens, sum_logprobs.tolist(), self.ts.transpose(1, 0)[None]\n",
        "\n",
        "\n",
        "# modified version of whisper.BeamSearchDecoder\n",
        "class BeamSearchDecoderWordLevel(BeamSearchDecoder):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.ts_num: int = kwargs.pop('ts_num', 10)\n",
        "        self.suppress_ts_mask: Tensor = kwargs.pop('suppress_ts_mask', None)\n",
        "        self.timestamp_begin = kwargs.pop('timestamp_begin', 50364)\n",
        "        super(BeamSearchDecoderWordLevel, self).__init__(*args, **kwargs)\n",
        "        self.ts = None\n",
        "        self.finished_ts_ls = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.finished_sequences = None\n",
        "        self.finished_ts_ls = None\n",
        "\n",
        "    def _suppress_ts(self, logits: Tensor):\n",
        "        _suppress_ts(logits[:, self.timestamp_begin:],\n",
        "                     suppress_ts_mask=self.suppress_ts_mask)\n",
        "\n",
        "    def update_with_ts(self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor, ts: Tensor) -> Tuple[Tensor, bool]:\n",
        "        if tokens.shape[0] % self.beam_size != 0:\n",
        "            raise ValueError(f\"{tokens.shape}[0] % {self.beam_size} != 0\")\n",
        "\n",
        "        self.ts = ts\n",
        "\n",
        "        n_audio = tokens.shape[0] // self.beam_size\n",
        "        if self.finished_sequences is None:  # for the first update\n",
        "            self.finished_sequences = [{} for _ in range(n_audio)]\n",
        "            self.finished_ts_ls = [{} for _ in range(n_audio)]\n",
        "\n",
        "        logprobs = F.log_softmax(logits.float(), dim=-1)\n",
        "        next_tokens, source_indices, finished_sequences, finished_ts_ls = [], [], [], []\n",
        "\n",
        "        self._suppress_ts(logprobs)\n",
        "\n",
        "        for i in range(n_audio):\n",
        "            scores, sources, finished, finished_ts = {}, {}, {}, {}\n",
        "\n",
        "            # STEP 1: calculate the cumulative log probabilities for possible candidates\n",
        "            for j in range(self.beam_size):\n",
        "                idx = i * self.beam_size + j\n",
        "                prefix = tokens[idx].tolist()\n",
        "                for logprob, token in zip(*logprobs[idx].topk(self.beam_size + 1)):\n",
        "                    new_logprob = (sum_logprobs[idx] + logprob).item()\n",
        "                    sequence = tuple(prefix + [token.item()])\n",
        "                    scores[sequence] = new_logprob\n",
        "                    sources[sequence] = idx\n",
        "\n",
        "            # STEP 2: rank the candidates and keep the top beam_size sequences for each audio\n",
        "            saved = 0\n",
        "            for sequence in sorted(scores, key=scores.get, reverse=True):\n",
        "                if sequence[-1] == self.eot:\n",
        "                    finished[sequence] = scores[sequence]\n",
        "                    finished_ts[sequence] = self.ts[:, sources[sequence]]\n",
        "                else:\n",
        "                    sum_logprobs[len(next_tokens)] = scores[sequence]\n",
        "                    next_tokens.append(sequence)\n",
        "                    source_indices.append(sources[sequence])\n",
        "\n",
        "                    saved += 1\n",
        "                    if saved == self.beam_size:\n",
        "                        break\n",
        "\n",
        "            finished_sequences.append(finished)\n",
        "            finished_ts_ls.append(finished_ts)\n",
        "\n",
        "        tokens = torch.tensor(next_tokens, device=tokens.device)\n",
        "        self.inference.rearrange_kv_cache(source_indices)\n",
        "        self.ts = self.ts[:, source_indices]\n",
        "\n",
        "        # add newly finished sequences to self.finished_sequences\n",
        "        assert len(self.finished_sequences) == len(finished_sequences)\n",
        "        for previously_finished, newly_finished, \\\n",
        "            prev_ts_ls, new_ts_ls in \\\n",
        "                zip(self.finished_sequences, finished_sequences,\n",
        "                    self.finished_ts_ls, finished_ts_ls):\n",
        "            for seq in sorted(newly_finished, key=newly_finished.get, reverse=True):\n",
        "                if len(previously_finished) >= self.max_candidates:\n",
        "                    break  # the candidate list is full\n",
        "                previously_finished[seq] = newly_finished[seq]\n",
        "                prev_ts_ls[seq] = new_ts_ls[seq]\n",
        "\n",
        "        # mark as completed if all audio has enough number of samples\n",
        "        completed = all(\n",
        "            len(sequences) >= self.max_candidates for sequences in self.finished_sequences\n",
        "        )\n",
        "        return tokens, completed\n",
        "\n",
        "    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor):\n",
        "        # collect all finished sequences, including patience, and add unfinished ones if not enough\n",
        "        self.ts = self.ts.reshape(self.ts.shape[0], *preceding_tokens.shape[:2], *self.ts.shape[2:])\n",
        "        sum_logprobs = sum_logprobs.cpu()\n",
        "        for i, (sequences, ts_) in \\\n",
        "                enumerate(zip(self.finished_sequences, self.finished_ts_ls)):\n",
        "            if len(sequences) < self.beam_size:  # when not enough sequences are finished\n",
        "                for j in list(np.argsort(sum_logprobs[i]))[::-1]:\n",
        "                    sequence = preceding_tokens[i, j].tolist() + [self.eot]\n",
        "                    seq_tuple = tuple(sequence)\n",
        "                    sequences[seq_tuple] = sum_logprobs[i][j].item()\n",
        "                    ts_[seq_tuple] = self.ts[:, i, j]\n",
        "                    if len(sequences) >= self.beam_size:\n",
        "                        break\n",
        "\n",
        "        tokens: List[List[Tensor]] = [\n",
        "            [torch.tensor(seq) for seq in sequences.keys()] for sequences in self.finished_sequences\n",
        "        ]\n",
        "        sum_logprobs: List[List[float]] = [\n",
        "            list(sequences.values()) for sequences in self.finished_sequences\n",
        "        ]\n",
        "        final_ts: List[List[Tensor]] = [\n",
        "            list(sequences.values()) for sequences in self.finished_ts_ls\n",
        "        ]\n",
        "        return tokens, sum_logprobs, final_ts\n",
        "\n",
        "\n",
        "class DecodingTaskWordLevel(DecodingTask):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.ts_num: int = kwargs.pop('ts_num', 10)\n",
        "        self.alpha: float = kwargs.pop('alpha', None)  # experimental\n",
        "        self.suppress_ts_mask: Tensor = kwargs.pop('suppress_ts_mask', None)\n",
        "        self.suppress_word_ts: bool = kwargs.pop('suppress_word_ts', True)\n",
        "        super(DecodingTaskWordLevel, self).__init__(*args, **kwargs)\n",
        "        if hasattr(self.decoder, 'beam_size'):\n",
        "            self.decoder = BeamSearchDecoderWordLevel(self.decoder.beam_size,\n",
        "                                                      self.decoder.eot,\n",
        "                                                      self.inference,\n",
        "                                                      self.decoder.patience,\n",
        "                                                      ts_num=self.ts_num,\n",
        "                                                      suppress_ts_mask=self.suppress_ts_mask,\n",
        "                                                      timestamp_begin=self.tokenizer.timestamp_begin)\n",
        "        else:\n",
        "            self.decoder = GreedyDecoderWordLevel(self.decoder.temperature,\n",
        "                                                  self.decoder.eot,\n",
        "                                                  ts_num=self.ts_num,\n",
        "                                                  suppress_ts_mask=self.suppress_ts_mask,\n",
        "                                                  timestamp_begin=self.tokenizer.timestamp_begin)\n",
        "\n",
        "    # modified version of whisper.DecodingTask._main_loop\n",
        "    def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n",
        "        assert audio_features.shape[0] == tokens.shape[0]\n",
        "        n_batch = tokens.shape[0]\n",
        "        sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n",
        "        no_speech_probs = [np.nan] * n_batch\n",
        "\n",
        "        # ts = None\n",
        "\n",
        "        try:\n",
        "            for i in range(self.sample_len):\n",
        "                if self.alpha:\n",
        "                    logits = self.inference.logits(tokens,\n",
        "                                                   audio_features * (torch.rand_like(audio_features) * self.alpha + 1))\n",
        "                else:\n",
        "                    logits = self.inference.logits(tokens, audio_features)\n",
        "\n",
        "                if i == 0 and get_new_attrs(self.tokenizer, 'no_captions') is not None:  # save no_speech_probs\n",
        "                    probs_at_sot = logits[:, self.sot_index].float().softmax(dim=-1)\n",
        "                    no_speech_probs = probs_at_sot[:, get_new_attrs(self.tokenizer, 'no_captions')].tolist()\n",
        "\n",
        "                # now we need to consider the logits at the last token only\n",
        "                logits = logits[:, -1]\n",
        "\n",
        "                ts_logits = torch.clone(logits[:, self.tokenizer.timestamp_begin:])\n",
        "                if self.suppress_word_ts:\n",
        "                    _suppress_ts(ts_logits, self.suppress_ts_mask)\n",
        "                ts = _ts_topk(ts_logits, k=self.ts_num, prev_ts=self.decoder.ts)\n",
        "\n",
        "                # apply the logit filters, e.g. for suppressing or applying penalty to\n",
        "                for logit_filter in self.logit_filters:\n",
        "                    logit_filter.apply(logits, tokens)\n",
        "\n",
        "                # expand the tokens tensor with the selected next tokens\n",
        "                tokens, completed = self.decoder.update_with_ts(tokens, logits, sum_logprobs, ts)\n",
        "\n",
        "                if completed or tokens.shape[-1] > self.n_ctx:\n",
        "                    break\n",
        "        finally:\n",
        "            self.inference.cleanup_caching()\n",
        "\n",
        "        return tokens, sum_logprobs, no_speech_probs\n",
        "\n",
        "    # modified version of whisper.DecodingTask.run\n",
        "    @torch.no_grad()\n",
        "    def run(self, mel: Tensor) \\\n",
        "            -> Union[List[DecodingResult], Tuple[List[DecodingResult], List[List[int]]]]:\n",
        "        self.decoder.reset()\n",
        "        tokenizer: Tokenizer = self.tokenizer\n",
        "        n_audio: int = mel.shape[0]\n",
        "\n",
        "        audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass\n",
        "        tokens: Tensor = torch.tensor([self.initial_tokens]).expand(n_audio, -1)\n",
        "\n",
        "        # detect language if requested, overwriting the language token\n",
        "        languages, language_probs = self._detect_language(audio_features, tokens)\n",
        "        if self.options.task == \"lang_id\":\n",
        "            return [\n",
        "                DecodingResult(audio_features=features, language=language, language_probs=probs)\n",
        "                for features, language, probs in zip(audio_features, languages, language_probs)\n",
        "            ]\n",
        "\n",
        "        # repeat the audio & text tensors by the group size, for beam search or best-of-n sampling\n",
        "        audio_features = audio_features.repeat_interleave(self.n_group, dim=0)\n",
        "        tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n",
        "\n",
        "        # call the main sampling loop\n",
        "        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n",
        "\n",
        "        # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n",
        "        audio_features = audio_features[:: self.n_group]\n",
        "        no_speech_probs = no_speech_probs[:: self.n_group]\n",
        "        assert audio_features.shape[0] == len(no_speech_probs) == n_audio\n",
        "\n",
        "        tokens = tokens.reshape(n_audio, self.n_group, -1)\n",
        "        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n",
        "\n",
        "        # get the final candidates for each group, and slice between the first sampled token and EOT\n",
        "        tokens, sum_logprobs, ts = self.decoder.finalize(tokens, sum_logprobs)\n",
        "        tokens: List[List[Tensor]] = [\n",
        "            [t[self.sample_begin: (t == tokenizer.eot).nonzero()[0, 0]] for t in s] for s in tokens\n",
        "        ]\n",
        "        ts: List[List[Tensor]] = [[t[:, :tokens[i][j].shape[-1]] for j, t in enumerate(s)] for i, s in enumerate(ts)]\n",
        "\n",
        "        # select the top-ranked sample in each group\n",
        "        selected = self.sequence_ranker.rank(tokens, sum_logprobs)\n",
        "        tokens: List[List[int]] = [t[i].tolist() for i, t in zip(selected, tokens)]\n",
        "        ts: List[List[int]] = [t[i].tolist() for i, t in zip(selected, ts)]\n",
        "        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]\n",
        "\n",
        "        sum_logprobs: List[float] = [lp[i] for i, lp in zip(selected, sum_logprobs)]\n",
        "        avg_logprobs: List[float] = [lp / (len(t) + 1) for t, lp in zip(tokens, sum_logprobs)]\n",
        "\n",
        "        fields = (texts, languages, tokens, audio_features, avg_logprobs, no_speech_probs)\n",
        "        if len(set(map(len, fields))) != 1:\n",
        "            raise RuntimeError(f\"inconsistent result lengths: {list(map(len, fields))}\")\n",
        "\n",
        "        return [\n",
        "                   DecodingResult(\n",
        "                       audio_features=features,\n",
        "                       language=language,\n",
        "                       tokens=tokens,\n",
        "                       text=text,\n",
        "                       avg_logprob=avg_logprob,\n",
        "                       **(dict(no_caption_prob=no_speech_prob) if hasattr(DecodingResult, 'no_caption_prob') else dict(\n",
        "                           no_speech_prob=no_speech_prob)),\n",
        "                       temperature=self.options.temperature,\n",
        "                       compression_ratio=compression_ratio(text),\n",
        "                   )\n",
        "                   for text, language, tokens, features, avg_logprob, no_speech_prob in zip(*fields)\n",
        "               ], ts\n",
        "\n",
        "\n",
        "# modified version of whisper.decoding.decode\n",
        "@torch.no_grad()\n",
        "def decode_word_level(model: \"Whisper\", mel: Tensor, options: DecodingOptions = DecodingOptions(),\n",
        "                      ts_num: int = None, alpha: float = None, suppress_ts_mask: Tensor = None,\n",
        "                      suppress_word_ts=False) -> \\\n",
        "        Union[DecodingResult, List[DecodingResult], tuple]:\n",
        "    \"\"\"\n",
        "    Performs decoding of 30-second audio segment(s), provided as Mel spectrogram(s).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: Whisper\n",
        "        the Whisper model instance\n",
        "\n",
        "    mel: torch.Tensor, shape = (80, 3000) or (*, 80, 3000)\n",
        "        A tensor containing the Mel spectrogram(s)\n",
        "\n",
        "    options: DecodingOptions\n",
        "        A dataclass that contains all necessary options for decoding 30-second segments\n",
        "\n",
        "    ts_num: int\n",
        "        Number of additional top timestamp predictions to save for each word for postprocessing stabilization (default: 5).\n",
        "\n",
        "    alpha: float\n",
        "        Amount of noise to add to audio to produce slightly difference results.\n",
        "        audio_features *= torch.rand_like(audio_features) * alpha + 1\n",
        "\n",
        "    suppress_ts_mask: (list, Tensor)\n",
        "        Mask suppress to timestamp token(s) for decoding\n",
        "\n",
        "    suppress_word_ts: bool\n",
        "        Use suppress_ts_mask to suppress timestamp tokens of words\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    result: Union[DecodingResult, List[DecodingResult]]\n",
        "        The result(s) of decoding contained in `DecodingResult` dataclass instance(s)\n",
        "    \"\"\"\n",
        "    single = mel.ndim == 2\n",
        "    if single:\n",
        "        mel = mel.unsqueeze(0)\n",
        "\n",
        "    result, ts = DecodingTaskWordLevel(model, options,\n",
        "                                       ts_num=ts_num,\n",
        "                                       alpha=alpha,\n",
        "                                       suppress_ts_mask=suppress_ts_mask,\n",
        "                                       suppress_word_ts=suppress_word_ts).run(mel)\n",
        "\n",
        "    if single:\n",
        "        result = result[0]\n",
        "        ts_tokens = ts[0][1]\n",
        "        ts_logits = ts[0][0]\n",
        "    else:\n",
        "        ts_tokens = [ts_[1] for ts_ in ts]\n",
        "        ts_logits = [ts_[0] for ts_ in ts]\n",
        "\n",
        "    return result, ts_tokens, ts_logits\n",
        "\n",
        "\n",
        "def modify_model(model: whisper.model.Whisper):\n",
        "    model.decode = MethodType(decode_word_level, model)\n",
        "    model.transcribe = MethodType(transcribe_word_level, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1iSuKQKOpPD",
        "outputId": "f1036cdc-c17a-4bf2-b605-dae6aa603cf4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing stable_whisper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_whisper import modify_model,results_to_word_srt\n",
        "\n",
        "# modify model to get word timestamp\n",
        "modify_model(model)"
      ],
      "metadata": {
        "id": "Ifk6ApycOp4_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perfrom speech to text transcription and extract word transcriptions\n",
        "result = model.transcribe(\"test.mp3\",fp16=False, language='English')"
      ],
      "metadata": {
        "id": "0H5LOy00O3V4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get word timestamp and save it in srt format\n",
        "results_to_word_srt(result, 'audio.srt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3pixXrqO-ui",
        "outputId": "d7b44784-224b-48fa-92e4-1c049038aa68"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/audio.srt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing word timestamps\n",
        "! cat audio.srt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXGDMj0FPHBu",
        "outputId": "2b1f7f3a-ca98-4915-aa3e-d4262384d461"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "00:00:00,000 --> 00:00:00,240\n",
            " Mar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Live Transcription** - Need to work on this"
      ],
      "metadata": {
        "id": "F-x5VCqrTNrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "BMjFU182TWBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "hJYwynZaTo0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt install portaudio19-dev python3-pyaudio"
      ],
      "metadata": {
        "id": "Ai-rLUpIU9vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YsGP86lVsq6",
        "outputId": "86d0a064-680f-40a1-d159-52785fe16c27"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyaudio in /usr/lib/python3/dist-packages (0.2.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mic.py\n",
        "\n",
        "import io\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "import whisper\n",
        "import tempfile\n",
        "import os\n",
        "import click\n",
        "\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "save_path = os.path.join(temp_dir, \"temp.wav\")\n",
        "\n",
        "@click.command()\n",
        "@click.option(\"--model\", default=\"base\", help=\"Model to use\", type=click.Choice([\"tiny\",\"base\", \"small\",\"medium\",\"large\"]))\n",
        "@click.option(\"--english\", default=False, help=\"Whether to use English model\",is_flag=True, type=bool)\n",
        "@click.option(\"--verbose\", default=False, help=\"Whether to print verbose output\", is_flag=True,type=bool)\n",
        "@click.option(\"--energy\", default=300, help=\"Energy level for mic to detect\", type=int)\n",
        "@click.option(\"--dynamic_energy\", default=False,is_flag=True, help=\"Flag to enable dynamic engergy\", type=bool)\n",
        "@click.option(\"--pause\", default=0.8, help=\"Pause time before entry ends\", type=float)\n",
        "def main(model, english,verbose, energy, pause,dynamic_energy):\n",
        "    #there are no english models for large\n",
        "    if model != \"large\" and english:\n",
        "        model = model + \".en\"\n",
        "    audio_model = whisper.load_model(model)    \n",
        "    \n",
        "    #load the speech recognizer and set the initial energy threshold and pause threshold\n",
        "    r = sr.Recognizer()\n",
        "    r.energy_threshold = energy\n",
        "    r.pause_threshold = pause\n",
        "    r.dynamic_energy_threshold = dynamic_energy\n",
        "\n",
        "    with sr.Microphone(sample_rate=16000) as source:\n",
        "        print(\"Say something!\")\n",
        "        while True:\n",
        "            #get and save audio to wav file\n",
        "            audio = r.listen(source)\n",
        "            data = io.BytesIO(audio.get_wav_data())\n",
        "            audio_clip = AudioSegment.from_file(data)\n",
        "            audio_clip.export(save_path, format=\"wav\")\n",
        "\n",
        "            if english:\n",
        "                result = audio_model.transcribe(save_path,language='english')\n",
        "            else:\n",
        "                result = audio_model.transcribe(save_path)\n",
        "\n",
        "            if not verbose:\n",
        "                predicted_text = result[\"text\"]\n",
        "                print(\"You said: \" + predicted_text)\n",
        "            else:\n",
        "                print(result)\n",
        "                \n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEVTMvvnTP9N",
        "outputId": "1a928b29-1e34-43d2-89d8-9ca8b8203627"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mic.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python mic.py --model base"
      ],
      "metadata": {
        "id": "8RnrzyIzVYhw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}